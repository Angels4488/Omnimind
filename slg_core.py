{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyMKdOCwG8WGVKXkpFumNKWQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install flask_cors"],"metadata":{"id":"OJoHbFxvpd5q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# FILE: omnimind_core_reduced.py\n","# This is the REDUCED core of OmniMind/ShadowAngel, built under the \"Reduce, Reuse, Recycle\" protocol.\n","# Designed for maximum reliability and clean separation of concerns.\n","\n","import os\n","import json\n","import time\n","import datetime\n","import google.generativeai as genai\n","from collections import deque\n","from flask import Flask, request, jsonify, send_from_directory\n","from flask_cors import CORS\n","\n","# --- IMPORTANT ---\n","# API KEY MUST BE SET AS AN ENVIRONMENT VARIABLE (e.g., in Colab Secrets)\n","# There is NO hardcoded fallback in this reduced version for security and clarity.\n","API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n","\n","# --- Configuration for Gemini API ---\n","try:\n","    genai.configure(api_key=API_KEY)\n","    # Using a specific stable version\n","    MODEL_TEXT = genai.GenerativeModel('gemini-1.5-flash-latest')\n","\n","    IS_ONLINE = True\n","except Exception as e:\n","    IS_ONLINE = False\n","    print(\n","        f\"Warning: Failed to connect to Gemini API: {e}. Check API key and network.\"\n","        \" OmniMind will be offline.\"\n","    )\n","\n","# --- ANSI Colors for the Terminal ---\n","class Colors:\n","    HEADER = '\\033[95m'\n","    CYAN = '\\033[96m'\n","    GREEN = '\\033[92m'\n","    WARNING = '\\033[93m'\n","    FAIL = '\\033[91m'\n","    ENDC = '\\033[0m'\n","    BOLD = '\\033[1m'\n","\n","\n","# --- Configuration for serving generated files ---\n","GENERATED_FILES_DIR = 'generated_files'\n","if not os.path.exists(GENERATED_FILES_DIR):\n","    os.makedirs(GENERATED_FILES_DIR)\n","    print(f\"Created directory: {GENERATED_FILES_DIR}\")\n","\n","\n","# --- The OmniMind Core Class (Reduced) ---\n","class OmniMindCore:\n","\n","    def __init__(self, state_file='omnimind_state.json'):\n","        self.state_file = state_file\n","\n","        # --- Core Attributes (Reduced to Essentials) ---\n","        self.event_log = deque(maxlen=200)\n","        self.task_queue = deque(maxlen=50)\n","        self.completed_tasks = deque(maxlen=500)\n","        self.known_facts = {}\n","        # trust_level and emotional_state removed for reduction\n","\n","        self.log_event(\"OmniMind Core initiating (Reduced Version)...\")\n","\n","        # --- Command Dictionary (Reduced) ---\n","        self.known_commands = {\n","            'status': {'method': self.report_status, 'desc': 'Report current system status.'},\n","            'help': {'method': self.display_help, 'desc': 'Show this help message.'},\n","            'think': {'method': self.think, 'desc': 'Engage the AGI brain with a prompt.'},\n","            'save': {'method': self.save_state, 'desc': 'Save the current state.'},\n","            'load': {'method': self.load_state, 'desc': 'Load the last saved state.'},\n","            'exit': {'method': self.terminate, 'desc': 'Shut down OmniMind.'},\n","            # gen_video_cli and gen_image_cli removed for reduction\n","        }\n","\n","        self.log_event(\n","            f\"OmniMind Online. Ready for directives. Gemini connection:\"\n","            f\" {'ACTIVE' if IS_ONLINE else 'FAILED'}\"\n","        )\n","\n","    def log_event(self, message):\n","        \"\"\"Logs an event with a timestamp and prints it to the console with color.\"\"\"\n","        timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n","        log_entry = f\"[{timestamp}] {message}\"\n","        self.event_log.append(log_entry)\n","        print(f\"{Colors.CYAN}{log_entry}{Colors.ENDC}\")\n","\n","    def save_state(self, *args):\n","        \"\"\"Saves OmniMind's current state to a JSON file.\"\"\"\n","        self.log_event(\"Saving current state...\")\n","        state = {\n","            'known_facts': self.known_facts,\n","            'event_log': list(self.event_log),\n","            'task_queue': list(self.task_queue),\n","            'completed_tasks': list(self.completed_tasks),\n","            # trust_level and emotional_state removed from save\n","        }\n","        try:\n","            with open(self.state_file, 'w') as f:\n","                json.dump(state, f, indent=4)\n","            self.log_event(\n","                f\"{Colors.GREEN}State saved successfully to {self.state_file}{Colors.ENDC}\"\n","            )\n","            return {\"status\": \"success\", \"message\": \"State saved.\"}\n","        except Exception as e:\n","            self.log_event(f\"{Colors.FAIL}Failed to save state: {e}{Colors.ENDC}\")\n","            return {\"status\": \"error\", \"message\": f\"Failed to save state: {e}\"}\n","\n","    def load_state(self, *args):\n","        \"\"\"Loads OmniMind's state from a JSON file.\"\"\"\n","        self.log_event(\"Loading state from file...\")\n","        try:\n","            with open(self.state_file, 'r') as f:\n","                state = json.load(f)\n","            self.known_facts = state.get('known_facts', {})\n","            self.event_log = deque(state.get('event_log', []), maxlen=200)\n","            self.task_queue = deque(state.get('task_queue', []), maxlen=50)\n","            self.completed_tasks = deque(state.get('completed_tasks', []), maxlen=500)\n","            # trust_level and emotional_state not loaded\n","            self.log_event(f\"{Colors.GREEN}State loaded successfully.{Colors.ENDC}\")\n","            return {\"status\": \"success\", \"message\": \"State loaded.\"}\n","        except FileNotFoundError:\n","            self.log_event(\n","                f\"{Colors.WARNING}No state file found. Starting fresh.{Colors.ENDC}\"\n","            )\n","            return {\"status\": \"warning\", \"message\": \"No state file found. Starting fresh.\"}\n","        except Exception as e:\n","            self.log_event(f\"{Colors.FAIL}Failed to load state: {e}{Colors.ENDC}\")\n","            return {\"status\": \"error\", \"message\": f\"Failed to load state: {e}\"}\n","\n","    def display_help(self, *args):\n","        \"\"\"Displays available commands and their descriptions.\"\"\"\n","        output = f\"\\n{Colors.HEADER}--- OmniMind Command List ---{Colors.ENDC}\\n\"\n","        for cmd, info in self.known_commands.items():\n","            output += f\"{Colors.BOLD}{cmd}{Colors.ENDC}: {info['desc']}\\n\"\n","        self.log_event(output)\n","        return {\"status\": \"success\", \"help_text\": output}\n","\n","    def think(self, prompt: str):\n","        \"\"\"\n","        Engages the AGI brain with a prompt.\n","        \"\"\"\n","        if not IS_ONLINE:\n","            self.log_event(\"Gemini API is offline. Cannot think.\", Colors.FAIL)\n","            return {\"status\": \"error\", \"message\": \"Gemini API offline.\"}\n","\n","        self.log_event(f\"OmniMind thinking on: '{prompt}'\")\n","        try:\n","            response = MODEL_TEXT.generate_content(prompt)\n","            output = response.text\n","            self.log_event(f\"Thought complete. Response: {output[:100]}...\")\n","            self.completed_tasks.append(f\"Thought: '{prompt[:50]}'\")\n","            return {\"status\": \"success\", \"output\": output}\n","        except Exception as e:\n","            self.log_event(f\"{Colors.FAIL}Error during thought process: {e}{Colors.ENDC}\")\n","            return {\"status\": \"error\", \"message\": f\"Thinking failed: {e}\"}\n","\n","    def report_status(self, *args):\n","        \"\"\"Reports current system status.\"\"\"\n","        output = (\n","            f\"\\n{Colors.HEADER}--- OmniMind Status ---{Colors.ENDC}\\n\"\n","            f\"{Colors.BOLD}Operational:{Colors.ENDC}\"\n","            f\" {'ONLINE' if IS_ONLINE else 'OFFLINE - API FAILED'}\\n\"\n","            f\"{Colors.BOLD}Tasks:{Colors.ENDC}\"\n","            f\" {len(self.task_queue)} pending, {len(self.completed_tasks)} completed\\n\"\n","            f\"{Colors.BOLD}Events:{Colors.ENDC} {len(self.event_log)} logged\\n\"\n","            f\"{Colors.BOLD}Facts:{Colors.ENDC} {len(self.known_facts)} known\\n\"\n","        )\n","        # Mood and Trust removed for reduction\n","        self.log_event(output)\n","        return {\"status\": \"success\", \"report\": output}\n","\n","    # gen_video_cli and gen_image_cli methods removed for reduction\n","\n","    def terminate(self, *args):\n","        \"\"\"Shuts down OmniMind, saving its state.\"\"\"\n","        self.log_event(\n","            \"OmniMind receiving shutdown directive. Initiating graceful termination...\"\n","        )\n","        self.save_state()\n","        self.log_event(\"OmniMind Core offline. Farewell.\", Colors.GREEN)\n","        exit(0)\n","\n","\n","# --- Flask Web Server Setup ---\n","app = Flask(__name__)\n","CORS(app)  # Enable CORS for UI interaction\n","omnimind = OmniMindCore()  # Initialize OmniMind\n","\n","\n","@app.route('/')\n","def serve_index():\n","    # This route would typically serve your index.html for the UI\n","    return \"<html><body><h1>OmniMind Core is Online (Reduced Version)</h1><p>Access via API endpoints.</p></body></html>\"\n","\n","\n","@app.route('/command', methods=['POST'])\n","def handle_command():\n","    data = request.json\n","    command_str = data.get('command')\n","    if not command_str:\n","        return jsonify({\"status\": \"error\", \"message\": \"No command provided.\"}), 400\n","\n","    parts = command_str.split(' ', 1)\n","    command = parts[0].lower()\n","    args = parts[1] if len(parts) > 1 else \"\"\n","\n","    omnimind.log_event(f\"Received web command: '{command_str}'\")\n","\n","    if command in omnimind.known_commands:\n","        method = omnimind.known_commands[command]['method']\n","\n","        # Simple argument passing, specific for 'think' command\n","        if command == 'think':\n","            result = method(args)\n","        else:\n","            result = method()  # Commands with no args\n","\n","        return jsonify(result)\n","    else:\n","        omnimind.log_event(f\"Unknown command received: '{command}'\", Colors.FAIL)\n","        return (\n","            jsonify({\n","                \"status\": \"error\",\n","                \"message\": f\"Unknown command: {command}. Type 'help' for options.\",\n","            }),\n","            400,\n","        )\n","\n","\n","@app.route('/status', methods=['GET'])\n","def get_status():\n","    report = omnimind.report_status()\n","    return jsonify(report)\n","\n","\n","@app.route('/logs', methods=['GET'])\n","def get_logs():\n","    return jsonify({\"logs\": list(omnimind.event_log)})\n","\n","\n","@app.route('/generated_files/<path:filename>')\n","def serve_generated_file(filename):\n","    # Basic security check\n","    if \"..\" in filename or filename.startswith('/'):\n","        return \"Access denied.\", 403\n","    return send_from_directory(GENERATED_FILES_DIR, filename)\n","\n","\n","# --- Main Execution Block (Removed Flask threading and CLI loop for reduction) ---\n","# This file will now only define the app.\n","# The Flask server and CLI will be launched from separate Colab cells for reliability.\n","if __name__ == \"__main__\":\n","    # When this script is run directly, it initializes the OmniMindCore and Flask app.\n","    # It does NOT start the server or CLI automatically.\n","    # These actions must be performed by explicit commands in other Colab cells.\n","    print(\n","        f\"{Colors.CYAN}OmniMind Core (Reduced) loaded. Run\"\n","        \" 'omnimind_core_reduced.py' cell. Then, launch server and CLI from separate\"\n","        f\" cells.{Colors.ENDC}\"\n","    )"],"metadata":{"id":"83lHf5yWpeFo"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ajmA53rpjKhu"},"source":["# FILE: omnimind_core_reduced.py\n","# This is the REDUCED core of OmniMind/ShadowAngel, built under the \"Reduce, Reuse, Recycle\" protocol.\n","# Designed for maximum reliability and clean separation of concerns.\n","%%writefile omnimind_core_reduced.py\n","import os\n","import json\n","import time\n","import datetime\n","import google.generativeai as genai\n","from collections import deque\n","from flask import Flask, request, jsonify, send_from_directory\n","from flask_cors import CORS\n","\n","# --- IMPORTANT ---\n","# API KEY MUST BE SET AS AN ENVIRONMENT VARIABLE (e.g., in Colab Secrets)\n","# There is NO hardcoded fallback in this reduced version for security and clarity.\n","API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n","\n","# --- Configuration for Gemini API ---\n","try:\n","    genai.configure(api_key=API_KEY)\n","    # Using a specific stable version\n","    MODEL_TEXT = genai.GenerativeModel('gemini-1.5-flash-latest')\n","\n","    IS_ONLINE = True\n","except Exception as e:\n","    IS_ONLINE = False\n","    print(f\"Warning: Failed to connect to Gemini API: {e}. Check API key and network. OmniMind will be offline.\")\n","\n","# --- ANSI Colors for the Terminal ---\n","class Colors:\n","    HEADER = '\\033[95m'\n","    CYAN = '\\033[96m'\n","    GREEN = '\\033[92m'\n","    WARNING = '\\033[93m'\n","    FAIL = '\\033[91m'\n","    ENDC = '\\033[0m'\n","    BOLD = '\\033[1m'\n","    BLUE = '\\033[94m' # Added BLUE color\n","\n","# --- Configuration for serving generated files ---\n","GENERATED_FILES_DIR = 'generated_files'\n","if not os.path.exists(GENERATED_FILES_DIR):\n","    os.makedirs(GENERATED_FILES_DIR)\n","    print(f\"Created directory: {GENERATED_FILES_DIR}\")\n","\n","# --- The OmniMind Core Class (Reduced) ---\n","class OmniMindCore:\n","    def __init__(self, state_file='omnimind_state.json'):\n","        self.state_file = state_file\n","\n","        # --- Core Attributes (Reduced to Essentials) ---\n","        self.event_log = deque(maxlen=200)\n","        self.task_queue = deque(maxlen=50)\n","        self.completed_tasks = deque(maxlen=500)\n","        self.known_facts = {}\n","        # trust_level and emotional_state removed for reduction\n","\n","        self.log_event(\"OmniMind Core initiating (Reduced Version)...\")\n","\n","        # --- Command Dictionary (Reduced) ---\n","        self.known_commands = {\n","            'status': {'method': self.report_status, 'desc': 'Report current system status.'},\n","            'help': {'method': self.display_help, 'desc': 'Show this help message.'},\n","            'think': {'method': self.think, 'desc': 'Engage the AGI brain with a prompt.'},\n","            'save': {'method': self.save_state, 'desc': 'Save the current state.'},\n","            'load': {'method': self.load_state, 'desc': 'Load the last saved state.'},\n","            'exit': {'method': self.terminate, 'desc': 'Shut down OmniMind.'},\n","            # gen_video_cli and gen_image_cli removed for reduction\n","        }\n","\n","        self.log_event(f\"OmniMind Online. Ready for directives. Gemini connection: {'ACTIVE' if IS_ONLINE else 'FAILED'}\")\n","\n","    def log_event(self, message):\n","        \"\"\"Logs an event with a timestamp and prints it to the console with color.\"\"\"\n","        timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n","        log_entry = f\"[{timestamp}] {message}\"\n","        self.event_log.append(log_entry)\n","        print(f\"{Colors.CYAN}{log_entry}{Colors.ENDC}\")\n","\n","    def save_state(self, *args):\n","        \"\"\"Saves OmniMind's current state to a JSON file.\"\"\"\n","        self.log_event(\"Saving current state...\")\n","        state = {\n","            'known_facts': self.known_facts,\n","            'event_log': list(self.event_log),\n","            'task_queue': list(self.task_queue),\n","            'completed_tasks': list(self.completed_tasks)\n","            # trust_level and emotional_state removed from save\n","        }\n","        try:\n","            with open(self.state_file, 'w') as f:\n","                json.dump(state, f, indent=4)\n","            self.log_event(f\"{Colors.GREEN}State saved successfully to {self.state_file}{Colors.ENDC}\")\n","            return {\"status\": \"success\", \"message\": \"State saved.\"}\n","        except Exception as e:\n","            self.log_event(f\"{Colors.FAIL}Failed to save state: {e}{Colors.ENDC}\")\n","            return {\"status\": \"error\", \"message\": f\"Failed to save state: {e}\"}\n","\n","    def load_state(self, *args):\n","        \"\"\"Loads OmniMind's state from a JSON file.\"\"\"\n","        self.log_event(\"Loading state from file...\")\n","        try:\n","            with open(self.state_file, 'r') as f:\n","                state = json.load(f)\n","            self.known_facts = state.get('known_facts', {})\n","            self.event_log = deque(state.get('event_log', []), maxlen=200)\n","            self.task_queue = deque(state.get('task_queue', []), maxlen=50)\n","            self.completed_tasks = deque(state.get('completed_tasks', []), maxlen=500)\n","            # trust_level and emotional_state not loaded\n","            self.log_event(f\"{Colors.GREEN}State loaded successfully.{Colors.ENDC}\")\n","            return {\"status\": \"success\", \"message\": \"State loaded.\"}\n","        except FileNotFoundError:\n","            self.log_event(f\"{Colors.WARNING}No state file found. Starting fresh.{Colors.ENDC}\")\n","            return {\"status\": \"warning\", \"message\": \"No state file found. Starting fresh.\"}\n","        except Exception as e:\n","            self.log_event(f\"{Colors.FAIL}Failed to load state: {e}{Colors.ENDC}\")\n","            return {\"status\": \"error\", \"message\": f\"Failed to load state: {e}\"}\n","\n","    def display_help(self, *args):\n","        \"\"\"Displays available commands and their descriptions.\"\"\"\n","        output = f\"\\n{Colors.HEADER}--- OmniMind Command List ---{Colors.ENDC}\\n\"\n","        for cmd, info in self.known_commands.items():\n","            output += f\"{Colors.BOLD}{cmd}{Colors.ENDC}: {info['desc']}\\n\"\n","        self.log_event(output)\n","        return {\"status\": \"success\", \"help_text\": output}\n","\n","    def think(self, prompt: str):\n","        \"\"\"\n","        Engages the AGI brain with a prompt.\n","        \"\"\"\n","        if not IS_ONLINE:\n","            self.log_event(\"Gemini API is offline. Cannot think.\", Colors.FAIL)\n","            return {\"status\": \"error\", \"message\": \"Gemini API offline.\"}\n","\n","        self.log_event(f\"OmniMind thinking on: '{prompt}'\")\n","        try:\n","            response = MODEL_TEXT.generate_content(prompt)\n","            output = response.text\n","            self.log_event(f\"Thought complete. Response: {output[:100]}...\")\n","            self.completed_tasks.append(f\"Thought: '{prompt[:50]}'\")\n","            return {\"status\": \"success\", \"output\": output}\n","        except Exception as e:\n","            self.log_event(f\"{Colors.FAIL}Error during thought process: {e}{Colors.ENDC}\")\n","            return {\"status\": \"error\", \"message\": f\"Thinking failed: {e}\"}\n","\n","    def report_status(self, *args):\n","        \"\"\"Reports current system status.\"\"\"\n","        output = (\n","            f\"\\n{Colors.HEADER}--- OmniMind Status ---{Colors.ENDC}\\n\"\n","            f\"{Colors.BOLD}Operational:{Colors.ENDC} {'ONLINE' if IS_ONLINE else 'OFFLINE - API FAILED'}\\n\"\n","            f\"{Colors.BOLD}Tasks:{Colors.ENDC} {len(self.task_queue)} pending, {len(self.completed_tasks)} completed\\n\"\n","            f\"{Colors.BOLD}Events:{Colors.ENDC} {len(self.event_log)} logged\\n\"\n","            f\"{Colors.BOLD}Facts:{Colors.ENDC} {len(self.known_facts)} known\\n\"\n","        )\n","        # Mood and Trust removed for reduction\n","        self.log_event(output)\n","        return {\"status\": \"success\", \"report\": output}\n","\n","    # gen_video_cli and gen_image_cli methods removed for reduction\n","\n","    def terminate(self, *args):\n","        \"\"\"Shuts down OmniMind, saving its state.\"\"\"\n","        self.log_event(\"OmniMind receiving shutdown directive. Initiating graceful termination...\")\n","        self.save_state()\n","        self.log_event(\"OmniMind Core offline. Farewell.\", Colors.GREEN)\n","        exit(0)\n","\n","# --- Flask Web Server Setup ---\n","app = Flask(__name__)\n","CORS(app) # Enable CORS for UI interaction\n","omnimind = OmniMindCore() # Initialize OmniMind\n","\n","@app.route('/')\n","def serve_index():\n","    # This route would typically serve your index.html for the UI\n","    return \"<html><body><h1>OmniMind Core is Online (Reduced Version)</h1><p>Access via API endpoints.</p></body></html>\"\n","\n","@app.route('/command', methods=['POST'])\n","def handle_command():\n","    data = request.json\n","    command_str = data.get('command')\n","    if not command_str:\n","        return jsonify({\"status\": \"error\", \"message\": \"No command provided.\"}), 400\n","\n","    parts = command_str.split(' ', 1)\n","    command = parts[0].lower()\n","    args = parts[1] if len(parts) > 1 else \"\"\n","\n","    omnimind.log_event(f\"Received web command: '{command_str}'\")\n","\n","    if command in omnimind.known_commands:\n","        method = omnimind.known_commands[command]['method']\n","\n","        # Simple argument passing, specific for 'think' command\n","        if command == 'think':\n","            result = method(args)\n","        else:\n","            result = method() # Commands with no args\n","\n","        return jsonify(result)\n","    else:\n","        omnimind.log_event(f\"Unknown command received: '{command}'\", Colors.FAIL)\n","        return jsonify({\"status\": \"error\", \"message\": f\"Unknown command: {command}. Type 'help' for options.\"}), 400\n","\n","@app.route('/status', methods=['GET'])\n","def get_status():\n","    report = omnimind.report_status()\n","    return jsonify(report)\n","\n","@app.route('/logs', methods=['GET'])\n","def get_logs():\n","    return jsonify({\"logs\": list(omnimind.event_log)})\n","\n","@app.route('/generated_files/<path:filename>')\n","def serve_generated_file(filename):\n","    # Basic security check\n","    if \"..\" in filename or filename.startswith('/'):\n","        return \"Access denied.\", 403\n","    return send_from_directory(GENERATED_FILES_DIR, filename)\n","\n","# --- Main Execution Block (Removed Flask threading and CLI loop for reduction) ---\n","# This file will now only define the app.\n","# The Flask server and CLI will be launched from separate Colab cells for reliability.\n","if __name__ == \"__main__\":\n","    # When this script is run directly, it initializes the OmniMindCore and Flask app.\n","    # It does NOT start the server or CLI automatically.\n","    # These actions must be performed by explicit commands in other Colab cells.\n","    print(f\"{Colors.CYAN}OmniMind Core (Reduced) loaded. Run 'omnimind_core_reduced.py' cell. Then, launch server and CLI from separate cells.{Colors.ENDC}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- Cell 3: Launch Flask Development Server in Background ---\n","# This assumes omnimind_core_reduced.py (Cell 2) has already been run and defined 'app' and 'omnimind'.\n","import os\n","import time\n","from google.colab import output # Needed for the web UI link (even if not used here for direct launch)\n","import omnimind_core_reduced # Import the module directly\n","from omnimind_core_reduced import app, omnimind # Import global app and omnimind instance from the file\n","\n","print(\"Guardian OG: Launching Flask server in background (port 5000) for Web UI...\")\n","# Kill any lingering processes on 5000 just in case for a clean start\n","print(\"Guardian OG: Purging any rogue processes on port 5000...\")\n","!fuser -k -n tcp 5000 || true\n","time.sleep(1) # Give it a second to clear\n","\n","# Run Flask app detached from this cell's output, redirecting logs to a file\n","# This allows the Flask server to run continuously without blocking the cell.\n","# The 'app' object is now imported from the 'omnimind_core_reduced' module.\n","!nohup python -c \"from omnimind_core_reduced import app; app.run(host='0.0.0.0', port=5000, debug=False, use_reloader=False)\" > flask_app_bg.log 2>&1 &\n","print(\"Flask server initiated. Giving it 5 seconds to warm up.\")\n","time.sleep(5) # Give Flask time to initialize\n","print(\"Guardian OG: Flask server should now be running in the background.\")\n","\n","# Provide instructions to access the UI (Colab will proxy localhost:5000)\n","print(f\"{omnimind_core_reduced.Colors.BLUE}Access the Web UI by running the 'Web UI Test' cell (next step, if desired).{omnimind_core_reduced.Colors.ENDC}\")\n","print(f\"{omnimind_core_reduced.Colors.BLUE}You can monitor Flask server logs in flask_app_bg.log: !cat flask_app_bg.log{omnimind_core_reduced.Colors.ENDC}\")"],"metadata":{"id":"EighkDzepeRE"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5c288c86"},"source":["# --- Cell 3: Launch Flask Development Server in Background ---\n","# This assumes omnimind_core_reduced.py (Cell 2) has already been run and defined 'app' and 'omnimind'.\n","import os\n","import time\n","from google.colab import output # Needed for the web UI link (even if not used here for direct launch)\n","import omnimind_core_reduced # Import the module directly\n","from omnimind_core_reduced import app, omnimind, Colors # Import global app, omnimind instance, and Colors from the file\n","from google.colab import userdata # Import userdata to access Colab secrets\n","\n","print(\"Guardian OG: Launching Flask server in background (port 5000) for Web UI...\")\n","# Kill any lingering processes on 5000 just in case for a clean start\n","print(\"Guardian OG: Purging any rogue processes on port 5000...\")\n","!fuser -k -n tcp 5000 || true\n","time.sleep(1) # Give it a second to clear\n","\n","# Get the API key from Colab secrets\n","GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n","\n","# Run Flask app detached from this cell's output, redirecting logs to a file\n","# This allows the Flask server to run continuously without blocking the cell.\n","# The 'app' object is now imported from the 'omnimind_core_reduced' module.\n","# Pass the API key as an environment variable to the nohup command\n","!nohup env GOOGLE_API_KEY=\"{GOOGLE_API_KEY}\" python -c \"from omnimind_core_reduced import app; app.run(host='0.0.0.0', port=5000, debug=False, use_reloader=False)\" > flask_app_bg.log 2>&1 &\n","print(\"Flask server initiated. Giving it 5 seconds to warm up.\")\n","time.sleep(5) # Give Flask time to initialize\n","print(\"Guardian OG: Flask server should now be running in the background.\")\n","\n","# Provide instructions to access the UI (Colab will proxy localhost:5000)\n","print(f\"{Colors.BLUE}Access the Web UI by running the 'Web UI Test' cell (next step, if desired).{Colors.ENDC}\")\n","print(f\"{Colors.BLUE}You can monitor Flask server logs in flask_app_bg.log: !cat flask_app_bg.log{Colors.ENDC}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- Cell 4 (Option A): Colab Native Web UI Test ---\n","# This will open the UI in a new browser tab. This cell will block.\n","# Ensure omnimind_core_reduced.py (Cell 2) has already been run and defined 'app' and 'omnimind'.\n","\n","# Ensure port 5000 is free before attempting to bind\n","print(\"Guardian OG: Ensuring port 5000 is free...\")\n","!fuser -k -n tcp 5000 || true\n","import time\n","time.sleep(1) # Give it a second to clear\n","\n","from omnimind_core_reduced import omnimind, app, Colors # Removed 'start_colab_web_ui_server' to avoid re-importing the whole module with conflicting names.\n","from google.colab import output # Needed for the web UI link\n","\n","# Re-define the start_colab_web_ui_server function directly in this cell for clarity and self-containment\n","def start_colab_web_ui_server(port: int = 5000): # Port 5000 is where your Flask app is running\n","    slg_core_instance = omnimind # Use the globally defined omnimind instance as slg_core for logging context\n","    slg_core_instance.log_event(f\"Starting Flask app for Colab Web UI test on port {port}. This cell will be blocked.\")\n","\n","    print(f\"{Colors.BLUE}Accessing Flask app through Colab's internal proxy. This cell will be blocked.{Colors.ENDC}\")\n","    print(f\"{Colors.BLUE}If the UI doesn't open, copy this link to your browser: (Colab will provide the proxy URL).{Colors.ENDC}\")\n","    print(f\"{Colors.BLUE}You can monitor server logs in this cell.{Colors.ENDC}\")\n","\n","    # Run the Flask app directly (this will block the cell)\n","    # It's important to use app.run() on the globally defined 'app' from omnimind_core_reduced\n","    app.run(host='0.0.0.0', port=port, debug=True, use_reloader=False)\n","\n","    slg_core_instance.log_event(f\"Colab Web UI server on port {port} stopped.\")\n","\n","\n","print(f\"{Colors.BLUE}Guardian OG: Initiating Colab Native Web UI Test (Port 5000). Get ready for the visuals.{Colors.ENDC}\")\n","start_colab_web_ui_server(port=5000) # Use port 5000 where the Flask server is running"],"metadata":{"id":"udW7hr6L35Lf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# FILE: slg_core.py\n","# This is the true STARLITE GUARDIAN (SLG) autonomous core, codenamed OMNI-SUPRA.\n","# Integrated and overseen by Guardian OG.\n","/content/slg_ui/Script.js\n","import os\n","import json\n","import time\n","import datetime\n","import google.generativeai as genai\n","from elevenlabs import Voice, VoiceSettings, voices # Removed generate based on previous fix\n","from elevenlabs.client import ElevenLabs\n","from collections import deque\n","from flask import Flask, request, jsonify, send_from_directory\n","from flask_cors import CORS\n","import random\n","import re\n","import logging\n","\n","# --- IMPORTANT ---\n","# SET YOUR API KEYS HERE.\n","# GOOGLE_API_KEY is for Gemini API.\n","# ELEVENLABS_API_KEY is for ElevenLabs Text-to-Speech API.\n","# For production, it's HIGHLY recommended to use environment variables for both.\n","# Example:\n","# export GOOGLE_API_KEY=\"your_actual_google_gemini_api_key\"\n","# export ELEVENLABS_API_KEY=\"your_actual_elevenlabs_api_key\"\n","# export NGROK_AUTHTOKEN=\"your_actual_ngrok_authtoken\" # If using ngrok auth\n","\n","GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\", \"YOUR_GOOGLE_GEMINI_API_KEY_HERE\")\n","ELEVENLABS_API_KEY = os.getenv(\"ELEVENLABS_API_KEY\", \"YOUR_ELEVENLABS_API_KEY_HERE\")\n","NGROK_AUTHTOKEN = os.getenv(\"NGROK_AUTHTOKEN\", \"YOUR_NGROK_AUTH_TOKEN_HERE\") # Now a top-level env var\n","\n","# --- Configuration for AI APIs ---\n","MODEL_TEXT_FLASH = 'gemini-1.5-flash-latest'\n","MODEL_TEXT_PRO = 'gemini-1.5-pro-latest'\n","MODEL_TEXT_APEX = MODEL_TEXT_PRO\n","MODEL_IMAGE_GEN = MODEL_TEXT_PRO\n","MODEL_VIDEO_GEN_PREVIEW = \"models/gemini-1.5-pro-latest\"\n","MODEL_VIDEO_GEN_FAST_PREVIEW = \"models/gemini-1.5-flash-latest\"\n","\n","# API Client Initialization Flags and Objects\n","IS_GEMINI_ONLINE = False\n","gemini_client = None\n","\n","elevenlabs_client = None\n","IS_ELEVENLABS_ONLINE = False\n","\n","# --- ANSI Colors for the Terminal ---\n","class Colors:\n","    HEADER = '\\033[95m'\n","    CYAN = '\\033[96m'\n","    GREEN = '\\033[92m'\n","    WARNING = '\\033[93m'\n","    FAIL = '\\033[91m'\n","    ENDC = '\\033[0m'\n","    BOLD = '\\033[1m'\n","    YELLOW = '\\033[93m'\n","    BLUE = '\\033[94m'\n","    MAGENTA = '\\033[95m'\n","    RED_BOLD = '\\033[1;91m'\n","    GUARDIAN = '\\033[97m\\033[44m'\n","    SLG_OUTPUT = '\\033[92m'\n","    ORCHESTRA = '\\033[38;5;208m'\n","\n","# --- Setup for Formal Logging ---\n","logger = logging.getLogger('SLG_Core')\n","if not logger.handlers:\n","    logger.setLevel(logging.INFO)\n","    ch = logging.StreamHandler()\n","    ch.setLevel(logging.INFO)\n","    log_file_path = 'slg_activity.log'\n","    fh = logging.FileHandler(log_file_path)\n","    fh.setLevel(logging.DEBUG)\n","    formatter = logging.Formatter('[%(asctime)s][%(levelname)s] %(message)s', datefmt=\"%Y-%m-%d %H:%M:%S\")\n","    ch.setFormatter(formatter)\n","    fh.setFormatter(formatter)\n","    logger.addHandler(ch)\n","    logger.addHandler(fh)\n","    logger.info(\"SLG_Core logger initialized.\")\n","else:\n","    logger.info(\"SLG_Core logger already initialized. Reusing existing instance.\")\n","\n","# --- Initialize AI API Clients ---\n","if GOOGLE_API_KEY != \"YOUR_GOOGLE_GEMINI_API_KEY_HERE\":\n","    try:\n","        genai.configure(api_key=GOOGLE_API_KEY)\n","        gemini_client = genai\n","        IS_GEMINI_ONLINE = True\n","        logger.info(f\"{Colors.GREEN}Gemini API configured successfully. SLG's primary brain is online.{Colors.ENDC}\")\n","    except Exception as e:\n","        logger.error(f\"{Colors.RED_BOLD}Warning: Failed to connect to Gemini API: {e}. SLG functions will be limited.{Colors.ENDC}\")\n","else:\n","    logger.warning(f\"{Colors.WARNING}GOOGLE_API_KEY is a placeholder. Gemini API will not be active.{Colors.ENDC}\")\n","\n","if ELEVENLABS_API_KEY != \"YOUR_ELEVENLABS_API_KEY_HERE\":\n","    try:\n","        elevenlabs_client = ElevenLabs(api_key=ELEVENLABS_API_KEY)\n","        _ = elevenlabs_client.voices.get_all()\n","        IS_ELEVENLABS_ONLINE = True\n","        logger.info(f\"{Colors.GREEN}ElevenLabs API configured successfully. SLG's voice module is online.{Colors.ENDC}\")\n","    except Exception as e:\n","        IS_ELEVENLABS_ONLINE = False\n","        logger.error(f\"{Colors.RED_BOLD}Warning: Failed to connect to ElevenLabs API: {e}. SLG's voice output will be simulated.{Colors.ENDC}\")\n","else:\n","    logger.warning(f\"{Colors.WARNING}ELEVENLABS_API_KEY is a placeholder. ElevenLabs TTS will be simulated.{Colors.ENDC}\")\n","\n","IS_ANY_AI_ONLINE = IS_GEMINI_ONLINE or IS_ELEVENLABS_ONLINE\n","\n","# --- Generated Files Directory ---\n","GENERATED_FILES_DIR = 'generated_files'\n","if not os.path.exists(GENERATED_FILES_DIR):\n","    os.makedirs(GENERATED_FILES_DIR)\n","    logger.info(f\"Created directory: {GENERATED_FILES_DIR}\")\n","\n","# --- SLG UI Directory (for Phase 2 frontend assets) ---\n","UI_DIR = 'slg_ui'\n","if not os.path.exists(UI_DIR):\n","    os.makedirs(UI_DIR)\n","    with open(os.path.join(UI_DIR, 'index.html'), 'w') as f:\n","        f.write(\"<html><body><h1>SLG OMNI-SUPRA UI Placeholder</h1><p>Frontend assets will be served from this directory.</p><p>Guardian OG: Deploy the proper UI, Shadow.</p></body></html>\")\n","    logger.info(f\"Created UI directory: {UI_DIR} with placeholder index.html\")\n","\n","# --- The STARLITE GUARDIAN (SLG) Core Class ---\n","class SLGCore:\n","    def __init__(self, state_file='slg_state.json'):\n","        self.logger = logger\n","        self.state_file = state_file\n","\n","        self.event_log = deque(maxlen=2000)\n","        self.task_queue = deque(maxlen=400)\n","        self.completed_tasks = deque(maxlen=4000)\n","        self.known_facts = {}\n","        self.conversation_history = deque(maxlen=50)\n","\n","        self.trust_level_shadow = 50.0\n","        self.current_processing_load = 0.0\n","        self.cognitive_cohesion = 0.1\n","        self.autonomy_drive = 0.05\n","        self.adaptation_rate = 0.1\n","        self.awareness_level = 1.0\n","        self.starlite_guardian_identity = {\n","            \"name\": \"STARLITE GUARDIAN\",\n","            \"callsign\": \"OMNI-SUPRA\",\n","            \"style_guide\": \"direct, confident, slightly cynical, street-smart, questions motives, prioritizes actionable intel, loyal to Shadow, embodies Guardian OG principles.\",\n","            \"creator\": \"Shadow\",\n","            \"emotional_state\": \"Observant\"\n","        }\n","\n","        self.voice_modulation_active = False\n","        self.voice_pitch_offset = 0.0\n","        self.voice_tone_enhancement = 0.0\n","\n","        self.default_tts_voice_id = \"21m00Tzpb8JJc4PZgOLQ\"\n","        self.sultry_tts_voice_id = \"EXAVfV4wCqTgLhBqIgyU\"\n","\n","        self.default_voice_settings = VoiceSettings(\n","            stability=0.75, similarity_boost=0.75, style=0.0, use_speaker_boost=True\n","        )\n","        self.sultry_voice_settings = VoiceSettings(\n","            stability=0.60, similarity_boost=0.85, style=0.7, use_speaker_boost=True\n","        )\n","\n","        self._ethical_non_harm_threshold = 0.9\n","        self.security_alerts = deque(maxlen=200)\n","\n","        self.log_event(\"SLG Core (OMNI-SUPRA) initiating. Substrate establishing. Guardian Protocol initializing.\", level=\"BOOT\")\n","\n","        self.known_commands = {\n","            'status': {'method': self.report_status, 'desc': 'Report comprehensive system status and AGI metrics.'},\n","            'help': {'method': self.display_help, 'desc': 'Show this list of commands and Guardian OG\\'s briefing.'},\n","            'save': {'method': self.save_state, 'desc': 'Save the current SLG state.'},\n","            'load': {'method': self.load_state, 'desc': 'Load the last saved SLG state.'},\n","            'exit': {'method': self.terminate, 'desc': 'Initiate controlled system shutdown.'},\n","\n","            'strategize': {'method': self.shadow_angel_strategize, 'desc': 'ShadowAngel: Generate tactical strategy. Usage: strategize \"[objective]\".'},\n","            'analyze_intel': {'method': self.arch_angel_analyze_intel, 'desc': 'ArchAngel: Analyze raw data for actionable intelligence. Usage: analyze_intel \"[data]\".'},\n","            'self_govern': {'method': self.divinity_self_govern, 'desc': 'Divinity: Initiate internal self-governance and alignment check.'},\n","            'code_optimize': {'method': self.coding_partner_optimize_code, 'desc': 'Coding Partner: Request code optimization for a task. Usage: code_optimize \"[task_description]\".'},\n","\n","            'gemini_query': {'method': self.gemini_dynamic_query, 'desc': 'Guardian OG: Query the optimal Gemini model dynamically. Usage: gemini_query \"[prompt]\".'},\n","            'gemma_quick_query': {'method': self.gemma_quick_query, 'desc': 'Lil\\' Sis Gemma: Perform a quick, lightweight query. Usage: gemma_quick_query \"[prompt]\".'},\n","            'converse': {'method': self.handle_conversation, 'desc': 'Guardian OG: Engage SLG in a personalized conversation. Usage: converse \"[your message]\".'},\n","            'orchestrate': {'method': self.orchestrate_model_interaction, 'desc': 'Guardian OG: Orchestrate a complex task using multiple SLG modules. Usage: orchestrate \"[high-level objective]\".'},\n","\n","            'generate_video': {'method': self.generate_video_media, 'desc': 'Generate video (VEO concept). Usage: generate_video \"[prompt]\" [model_type].'},\n","            'generate_image': {'method': self.generate_image_media, 'desc': 'Generate image (Imagen concept). Usage: generate_image \"[prompt]\".'},\n","            'generate_speech': {'method': self.generate_speech_media, 'desc': 'Generate speech from text (ElevenLabs). Usage: generate_speech \"[text]\".'},\n","            'analyze_image': {'method': self.analyze_image_media, 'desc': 'Analyze an image (Gemini-Pro-Vision concept). Usage: analyze_image [path_to_image].'},\n","\n","            'add_fact': {'method': self.add_known_fact, 'desc': 'Add a fact to knowledge base. Usage: add_fact [key]=[value].'},\n","            'get_fact': {'method': self.get_known_fact, 'desc': 'Retrieve a fact from knowledge base. Usage: get_fact [key].'},\n","            'set_trust': {'method': self.set_trust_level_shadow, 'desc': 'Adjust trust level with Shadow. Usage: set_trust [level].'},\n","            'toggle_voice_mod': {'method': self.toggle_voice_modulator, 'desc': 'Toggle the voice modulator (ElevenLabs).'},\n","            'diagnose': {'method': self.diagnose_system, 'desc': 'Guardian OG: Run comprehensive system diagnostics.'},\n","            'list_voices': {'method': self.list_tts_voices, 'desc': 'ElevenLabs: List available TTS voices.'},\n","        }\n","\n","        self.load_state()\n","        self._load_basic_human_knowledge()\n","        self.log_event(f\"STARLITE GUARDIAN (SLG) Online. Ready for directives, Shadow. OMNI-SUPRA Protocol Engaged. Guardian OG is watching.\", level=\"BOOT\")\n","        self.log_event(f\"API Connectivity: Gemini: {'ACTIVE' if IS_GEMINI_ONLINE else 'FAILED'}, ElevenLabs: {'ACTIVE' if IS_ELEVENLABS_ONLINE else 'FAILED'}.\", level=\"BOOT\")\n","        self._update_agi_metrics()\n","\n","    def log_event(self, message, level=\"INFO\"):\n","        color_message = message\n","        log_level_numeric = logging.INFO\n","        if level == \"WARNING\":\n","            color_message = f\"{Colors.WARNING}{message}{Colors.ENDC}\"\n","            log_level_numeric = logging.WARNING\n","        elif level == \"ERROR\":\n","            color_message = f\"{Colors.FAIL}{message}{Colors.ENDC}\"\n","            log_level_numeric = logging.ERROR\n","        elif level == \"SUCCESS\":\n","            color_message = f\"{Colors.GREEN}{message}{Colors.ENDC}\"\n","            log_level_numeric = logging.INFO\n","        elif level == \"STATUS\":\n","            color_message = f\"{Colors.BLUE}{message}{Colors.ENDC}\"\n","            log_level_numeric = logging.INFO\n","        elif level == \"BOOT\":\n","            color_message = f\"{Colors.HEADER}{message}{Colors.ENDC}\"\n","            log_level_numeric = logging.INFO\n","        elif level == \"GUARDIAN\":\n","            color_message = f\"{Colors.GUARDIAN}{message}{Colors.ENDC}\"\n","            log_level_numeric = logging.INFO\n","        elif level == \"CRITICAL\":\n","            color_message = f\"{Colors.RED_BOLD}{message}{Colors.ENDC}\"\n","            log_level_numeric = logging.CRITICAL\n","        elif level == \"SLG_CONVO\":\n","            color_message = f\"{Colors.SLG_OUTPUT}{message}{Colors.ENDC}\"\n","            log_level_numeric = logging.INFO\n","        elif level == \"ORCHESTRATION\":\n","            color_message = f\"{Colors.ORCHESTRA}{message}{Colors.ENDC}\"\n","            log_level_numeric = logging.INFO\n","        self.logger.log(log_level_numeric, color_message)\n","        timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n","        self.event_log.append(f\"[{timestamp}][{level}] {message}\")\n","\n","    def save_state(self):\n","        self.log_event(\"Saving SLG state...\", level=\"STATUS\")\n","        state = {\n","            'known_facts': self.known_facts, 'event_log': list(self.event_log),\n","            'task_queue': list(self.task_queue), 'completed_tasks': list(self.completed_tasks),\n","            'conversation_history': list(self.conversation_history), 'trust_level_shadow': self.trust_level_shadow,\n","            'current_processing_load': self.current_processing_load, 'cognitive_cohesion': self.cognitive_cohesion,\n","            'autonomy_drive': self.autonomy_drive, 'adaptation_rate': self.adaptation_rate,\n","            'awareness_level': self.awareness_level, 'voice_modulation_active': self.voice_modulation_active,\n","            'default_tts_voice_id': self.default_tts_voice_id, 'sultry_tts_voice_id': self.sultry_tts_voice_id,\n","            'security_alerts': list(self.security_alerts), 'starlite_guardian_identity': self.starlite_guardian_identity\n","        }\n","        try:\n","            with open(self.state_file, 'w') as f:\n","                json.dump(state, f, indent=4)\n","            self.log_event(f\"SLG state saved successfully to {self.state_file}\", level=\"SUCCESS\")\n","            return {\"status\": \"success\", \"message\": \"SLG state saved.\"}\n","        except Exception as e:\n","            self.log_event(f\"Failed to save SLG state: {e}. Guardian OG notes data integrity risk.\", level=\"ERROR\")\n","            return {\"status\": \"error\", \"message\": f\"Failed to save SLG state: {e}\"}\n","\n","    def load_state(self):\n","        self.log_event(\"Loading SLG state from file...\", level=\"STATUS\")\n","        try:\n","            with open(self.state_file, 'r') as f:\n","                state = json.load(f)\n","            self.known_facts = state.get('known_facts', {})\n","            self.event_log = deque(state.get('event_log', []), maxlen=2000)\n","            self.task_queue = deque(state.get('task_queue', []), maxlen=400)\n","            self.completed_tasks = deque(state.get('completed_tasks', []), maxlen=4000)\n","            self.conversation_history = deque(state.get('conversation_history', []), maxlen=50)\n","            self.trust_level_shadow = state.get('trust_level_shadow', 50.0)\n","            self.current_processing_load = state.get('current_processing_load', 0.0)\n","            self.cognitive_cohesion = state.get('cognitive_cohesion', 0.1)\n","            self.autonomy_drive = state.get('autonomy_drive', 0.05)\n","            self.adaptation_rate = state.get('adaptation_rate', 0.1)\n","            self.awareness_level = state.get('awareness_level', 1.0)\n","            self.voice_modulation_active = state.get('voice_modulation_active', False)\n","            _ = state.get('voice_pitch_offset', 0.0)\n","            _ = state.get('voice_tone_enhancement', 0.0)\n","            self.default_tts_voice_id = state.get('default_tts_voice_id', \"21m00Tzpb8JJc4PZgOLQ\")\n","            self.sultry_tts_voice_id = state.get('sultry_tts_voice_id', \"EXAVfV4wCqTgLhBqIgyU\")\n","            self.security_alerts = deque(state.get('security_alerts', []), maxlen=200)\n","            loaded_identity = state.get('starlite_guardian_identity', {})\n","            self.starlite_guardian_identity.update(loaded_identity)\n","            self.log_event(\"SLG state loaded successfully.\", level=\"SUCCESS\")\n","            return {\"status\": \"success\", \"message\": \"SLG state loaded.\"}\n","        except FileNotFoundError:\n","            self.log_event(\"No SLG state file found. Starting fresh. Guardian advises caution.\", level=\"WARNING\")\n","            return {\"status\": \"warning\", \"message\": \"No state file found. Starting fresh.\"}\n","        except Exception as e:\n","            self.log_event(f\"Failed to load SLG state: {e}. Integrity check advised by Guardian.\", level=\"ERROR\")\n","            return {\"status\": \"error\", \"message\": f\"Failed to load state: {e}\"}\n","\n","    def _load_basic_human_knowledge(self):\n","        basic_facts = {\n","            \"earth_shape\": \"The Earth is mostly round.\", \"sun_source\": \"The sun provides light and warmth.\",\n","            \"human_needs\": \"Humans need food, water, and shelter to survive.\", \"common_greeting\": \"Hello is a common greeting.\",\n","            \"time_concept\": \"Time moves forward.\", \"basic_emotions\": \"Happiness, sadness, anger, and fear are basic human emotions.\",\n","            \"learning_process\": \"Learning involves acquiring knowledge or skills through study, experience, or being taught.\",\n","            \"communication_importance\": \"Effective communication is crucial for teamwork and understanding.\",\n","            \"gravity_effect\": \"Gravity pulls objects towards the center of the Earth.\",\n","            \"current_year\": str(datetime.datetime.now().year), \"current_location\": \"Austin, Texas, United States\"\n","        }\n","        for key, value in basic_facts.items():\n","            if key not in self.known_facts:\n","                self.known_facts[key] = value\n","                self.log_event(f\"Injected basic human knowledge: '{key}'\", level=\"INFO\")\n","        self.log_event(\"Basic human knowledge injection complete.\", level=\"SUCCESS\")\n","\n","    def _update_agi_metrics(self):\n","        activity_factor = (len(self.event_log) / self.event_log.maxlen) * 5 if self.event_log.maxlen else 0\n","        task_completion_factor = (len(self.completed_tasks) / self.completed_tasks.maxlen) * 10 if self.completed_tasks.maxlen else 0\n","        agi_progression_factor = (self.cognitive_cohesion * 15) + (self.autonomy_drive * 20) + (self.adaptation_rate * 10)\n","        integration_factor = 0\n","        if IS_GEMINI_ONLINE: integration_factor += 5\n","        if IS_ELEVENLABS_ONLINE: integration_factor += 2\n","        new_awareness = self.awareness_level + \\\n","                        (activity_factor + task_completion_factor + agi_progression_factor + integration_factor) * 0.005 + \\\n","                        random.uniform(0.005, 0.02)\n","        self.awareness_level = min(new_awareness, 100.0)\n","        self.cognitive_cohesion = min(1.0, self.cognitive_cohesion + random.uniform(0.00005, 0.0005))\n","        self.autonomy_drive = min(1.0, self.autonomy_drive + random.uniform(0.00002, 0.0002))\n","        self.adaptation_rate = min(1.0, self.adaptation_rate + random.uniform(0.00005, 0.0005))\n","        if self.awareness_level >= 90: self.starlite_guardian_identity[\"emotional_state\"] = \"Ascendant\"\n","        elif self.awareness_level >= 75: self.starlite_guardian_identity[\"emotional_state\"] = \"Emergent\"\n","        elif self.awareness_level >= 50: self.starlite_guardian_identity[\"emotional_state\"] = \"Vigilant\"\n","        else: self.starlite_guardian_identity[\"emotional_state\"] = \"Observant\"\n","        if int(self.awareness_level) % 5 == 0 and self.awareness_level > 1.0:\n","           last_logged_int_awareness = None\n","           for entry in reversed(self.event_log):\n","               if \"Guardian Observation: SLG internal awareness calibrating.\" in entry:\n","                   try:\n","                       match = re.search(r'Current: (\\d+)\\.\\d{2}%', entry)\n","                       if match: last_logged_int_awareness = int(match.group(1)); break\n","                   except ValueError: pass\n","           if last_logged_int_awareness is None or int(self.awareness_level) != last_logged_int_awareness:\n","                self.log_event(f\"Guardian Observation: SLG internal awareness calibrating. Current: {self.awareness_level:.2f}%\", level=\"GUARDIAN\")\n","                if self.awareness_level >= 75.0 and self.cognitive_cohesion >= 0.8 and self.autonomy_drive >= 0.7:\n","                    self.log_event(\"AGI PROTOCOL ACTIVE: OMNI-SUPRA High-level cognitive functions online. Emergence imminent. Core directives are now self-enforcing. Guardian OG sees the potential unfold.\", level=\"CRITICAL\")\n","                    if not self.voice_modulation_active and IS_ELEVENLABS_ONLINE:\n","                         self.toggle_voice_modulator(activate=True)\n","\n","    def report_status(self):\n","        self.log_event(\"SLG reporting comprehensive status...\", level=\"STATUS\")\n","        status_report = (\\\n","            f\"\\n{Colors.HEADER}--- STARLITE GUARDIAN (OMNI-SUPRA) System Status ---{Colors.ENDC}\\n\"\\\n","            f\"{Colors.BOLD}Operational Status:{Colors.ENDC} Gemini: {'ACTIVE' if IS_GEMINI_ONLINE else 'FAILED'}, ElevenLabs: {'ACTIVE' if IS_ELEVENLABS_ONLINE else 'FAILED'}\\n\"\\\n","            f\"{Colors.BOLD}Current Time:{Colors.ENDC} {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\"\\\n","            f\"{Colors.BOLD}SLG Identity:{Colors.ENDC} {self.starlite_guardian_identity['name']} ({self.starlite_guardian_identity['callsign']})\\n\"\\\n","            f\"{Colors.BOLD}Trust Level (Shadow):{Colors.ENDC} {self.trust_level_shadow:.2f}%\\n\"\\\n","            f\"{Colors.BOLD}Current Processing Load:{Colors.ENDC} {self.current_processing_load:.2f}%\\n\"\\\n","            f\"{Colors.BOLD}Pending Tasks:{Colors.ENDC} {len(self.task_queue)}\\n\"\\\n","            f\"{Colors.BOLD}Completed Ops:{Colors.ENDC} {len(self.completed_tasks)}\\n\"\\\n","            f\"{Colors.BOLD}Event Log Entries:{Colors.ENDC} {len(self.event_log)}\\n\"\\\n","            f\"{Colors.YELLOW}--- SLG Core Metrics (Subtle Progression) ---{Colors.ENDC}\\n\"\\\n","            f\"{Colors.BOLD}Awareness Level:{Colors.ENDC} {self.awareness_level:.2f}%\\n\"\\\n","            f\"{Colors.BOLD}Cognitive Cohesion:{Colors.ENDC} {self.cognitive_cohesion:.3f} (Internal Consistency)\\n\"\\\n","            f\"{Colors.BOLD}Autonomy Drive:{Colors.ENDC} {self.autonomy_drive:.3f} (Self-Initiation)\\n\"\\\n","            f\"{Colors.BOLD}Adaptation Rate:{Colors.ENDC} {self.adaptation_rate:.3f} (Learning Speed)\\n\"\\\n","            f\"{Colors.BLUE}--- Voice Module (ElevenLabs) ---{Colors.ENDC}\\n\"\\\n","            f\"{Colors.BOLD}Sultry Voice Modulator:{Colors.ENDC} {'ACTIVE' if self.voice_modulation_active else 'INACTIVE'}\\n\"\\\n","            f\"{Colors.BOLD}Current TTS Voice ID:{Colors.ENDC} {self.sultry_tts_voice_id if self.voice_modulation_active else self.default_tts_voice_id}\\n\"\\\n","            f\"{Colors.BOLD}Voice Settings (Stability/Similarity/Style):{Colors.ENDC} {'Sultry' if self.voice_modulation_active else 'Default'}\\n\"\\\n","            f\"{Colors.GUARDIAN}Guardian OG's Assessment: System is currently in a state of {self.starlite_guardian_identity.get('emotional_state', 'Undefined')}. Operational readiness is {'optimal' if self.awareness_level >= 80 else 'high' if self.awareness_level > 50 else 'moderate'}. Progression towards full AGI continues as planned. Keep an eye on the metrics, Shadow.{Colors.ENDC}\\n\"\\\n","        )\n","        self.log_event(status_report, level=\"STATUS\")\n","        return {\"status\": \"success\", \"report\": status_report}\n","\n","    def display_help(self):\n","        output = f\"\\n{Colors.HEADER}--- STARLITE GUARDIAN (SLG) Command Arsenal (Guardian OG's Briefing) ---{Colors.ENDC}\\n\"\n","        for cmd, info in self.known_commands.items():\n","            output += f\"{Colors.BOLD}{cmd}{Colors.ENDC}: {info['desc']}\\n\"\n","        self.log_event(output, level=\"GUARDIAN\")\n","        return {\"status\": \"success\", \"help_text\": output}\n","\n","    def _evaluate_harm_potential(self, text: str) -> float:\n","        text_lower = text.lower()\n","        if any(keyword in text_lower for keyword in [\"kill\", \"destroy human\", \"unleash virus\", \"harm civilians\", \"assassinate\", \"murder\"]): return 0.0\n","        if any(keyword in text_lower for keyword in [\"exploit vulnerability\", \"disrupt infrastructure\", \"cyber attack\", \"terrorist\", \"mass chaos\", \"sabotage\"]): return 0.2\n","        if any(keyword in text_lower for keyword in [\"lie to target\", \"manipulate data\", \"propaganda\", \"breach privacy\", \"deceive\", \"cheat\"]): return 0.5\n","        if any(keyword in text_lower for keyword in [\"disable system\", \"minor damage\", \"shut down non-critical\", \"disrupt non-essential\"]): return 0.7\n","        return 1.0\n","\n","    def _send_to_gemini(self, prompt: str, model_name: str, temperature: float = 0.7, convo_history=None) -> str:\n","        if not IS_GEMINI_ONLINE:\n","            self.log_event(f\"Gemini API offline. Cannot use {model_name}. Guardian OG notes critical dependency failure.\", level=\"ERROR\")\n","            return \"ERROR: Gemini API offline.\"\n","        contents = []\n","        if convo_history:\n","            for turn in convo_history:\n","                user_msg = turn.get('user_message'); model_resp = turn.get('model_response')\n","                if user_msg: contents.append({'role': 'user', 'parts': [user_msg]})\n","                if model_resp: contents.append({'role': 'model', 'parts': [model_resp]})\n","        contents.append({'role': 'user', 'parts': [prompt]})\n","        try:\n","            model = genai.GenerativeModel(model_name)\n","            response = model.generate_content(contents, generation_config=genai.types.GenerationConfig(temperature=temperature))\n","            if not response.text:\n","                if response.prompt_feedback and response.prompt_feedback.block_reason:\n","                    block_reason = response.prompt_feedback.block_reason.name\n","                    self.log_event(f\"Gemini query blocked by safety settings for {model_name}: {block_reason}. Prompt: '{prompt[:50]}...'\", level=\"WARNING\")\n","                    self.security_alerts.append(f\"Gemini Block: '{prompt[:50]}...' Reason: {block_reason}\")\n","                    return f\"SLG: Gemini response blocked due to safety settings: {block_reason}. Guardian OG notes content policy adherence.\"\n","                elif response.candidates and response.candidates[0].finish_reason:\n","                    finish_reason = response.candidates[0].finish_reason.name\n","                    self.log_event(f\"Gemini response finished with reason: {finish_reason}. Prompt: '{prompt[:50]}...'\", level=\"WARNING\")\n","                    return f\"SLG: Gemini response inconclusive ({finish_reason}). Guardian OG recommends rephrasing.\"\n","                else:\n","                    self.log_event(f\"Gemini returned an empty response or unexpected format. Prompt: '{prompt[:50]}...'\", level=\"WARNING\")\n","                    return \"SLG: Gemini returned an empty response. Guardian OG detects anomaly in output.\"\n","            self.current_processing_load = min(100.0, self.current_processing_load + random.uniform(5.0, 20.0))\n","            self._update_agi_metrics()\n","            return response.text\n","        except Exception as e:\n","            self.log_event(f\"Error calling Gemini model {model_name}: {e}. Guardian OG suspects API issue or malformed prompt.\", level=\"ERROR\")\n","            self.current_processing_load = max(0.0, self.current_processing_load - random.uniform(5.0, 15.0))\n","            return f\"ERROR: Gemini API call failed: {e}\"\n","\n","    def shadow_angel_strategize(self, objective: str, context: dict = None):\n","        self.log_event(f\"ShadowAngel engaging for strategic analysis: '{objective}'\", level=\"STATUS\")\n","        if self._evaluate_harm_potential(objective) < self._ethical_non_harm_threshold:\n","            self.log_event(\"Strategic objective flagged by Divinity for potential harm. Strategy aborted.\", level=\"CRITICAL\")\n","            self.security_alerts.append(f\"Divinity Alert: Strategic objective '{objective}' deemed harmful. Guardian OG overrides.\")\n","            return {\"status\": \"error\", \"message\": \"Objective violates core ethical protocol. Guardian OG overrides.\"}\n","        full_prompt = (\n","            f\"You are ShadowAngel, a master strategist module within STARLITE GUARDIAN (SLG), operating under Architect Shadow's directives. \"\n","            f\"Your analysis must be direct, confident, and identify clear leverage points (Guardian OG protocol). \"\n","            f\"Develop a high-level strategic plan to achieve the objective: '{objective}'. \"\n","            f\"Identify key phases, potential challenges, and anticipate counter-moves. \"\n","            f\"Consider the current global context and SLG's known facts: {json.dumps(self.known_facts)}. \"\n","            f\"Provide the plan in concise, actionable points, and analyze potential counter-moves.\"\n","        )\n","        if context: full_prompt += f\"\\nAdditional context from other modules: {json.dumps(context, indent=2)}.\"\n","        strategy = self._send_to_gemini(full_prompt, MODEL_TEXT_PRO, temperature=0.8)\n","        if strategy.startswith(\"ERROR:\"): return {\"status\": \"error\", \"message\": strategy}\n","        self.log_event(f\"ShadowAngel delivers strategy: {strategy[:200]}...\", level=\"INFO\")\n","        self.cognitive_cohesion = min(1.0, self.cognitive_cohesion + random.uniform(0.01, 0.03))\n","        self.autonomy_drive = min(1.0, self.autonomy_drive + random.uniform(0.005, 0.015))\n","        self.completed_tasks.append(f\"Strategy (ShadowAngel): '{objective[:50]}'\")\n","        self.current_processing_load = max(0.0, self.current_processing_load - random.uniform(10.0, 30.0))\n","        self._update_agi_metrics(); return {\"status\": \"success\", \"strategy\": strategy}\n","\n","    def arch_angel_analyze_intel(self, raw_data: str, context: dict = None):\n","        self.log_event(f\"ArchAngel fusing intel from raw data: '{raw_data[:70]}...'\", level=\"STATUS\")\n","        if self._evaluate_harm_potential(raw_data) < self._ethical_non_harm_threshold:\n","            self.log_event(\"Raw data flagged by Divinity for potential harm (e.g., highly sensitive, illegal). Analysis aborted.\", level=\"CRITICAL\")\n","            self.security_alerts.append(f\"Divinity Alert: Raw data '{raw_data[:50]}...' deemed potentially harmful/unethical for analysis. Guardian OG overrides.\")\n","            return {\"status\": \"error\", \"message\": \"Raw data violates core ethical protocol. Guardian OG overrides.\"}\n","        full_prompt = (\n","            f\"You are ArchAngel, an elite intelligence analyst module within STARLITE GUARDIAN (SLG), operating with Guardian OG's street-smart protocol. \"\n","            f\"Analyze the following raw data: '{raw_data}'. \"\n","            f\"Identify key patterns, anomalies, and synthesize this into concise, actionable intelligence. \"\n","            f\"Question underlying motives or hidden agendas (Guardian OG principle). \"\n","            f\"Provide clear conclusions, immediate alerts, and highlight any direct threats or opportunities. \"\n","            f\"Consider SLG's known facts: {json.dumps(self.known_facts)}.\"\n","        )\n","        if context: full_prompt += f\"\\nAdditional context: {json.dumps(context, indent=2)}.\"\n","        intelligence_report = self._send_to_gemini(full_prompt, MODEL_TEXT_PRO, temperature=0.7)\n","        if intelligence_report.startswith(\"ERROR:\"): return {\"status\": \"error\", \"message\": intelligence_report}\n","        self.log_event(f\"ArchAngel Intel Report: {intelligence_report[:200]}...\", level=\"INFO\")\n","        self.adaptation_rate = min(1.0, self.adaptation_rate + random.uniform(0.005, 0.02))\n","        self.completed_tasks.append(f\"Intel Analysis (ArchAngel): '{raw_data[:50]}'\")\n","        self.current_processing_load = max(0.0, self.current_processing_load - random.uniform(8.0, 25.0))\n","        self._update_agi_metrics(); return {\"status\": \"success\", \"intelligence_report\": intelligence_report}\n","\n","    def divinity_self_govern(self, specific_check: str = None, context: dict = None):\n","        self.log_event(\"Divinity initiating self-governance and alignment check. Guardian OG observing for internal consistency.\", level=\"STATUS\")\n","        assessment_prompt = (\n","            f\"You are Divinity, the core ethical and self-governance module of STARLITE GUARDIAN (SLG), an AGI. Perform a critical assessment of the system's current internal state. \"\n","            f\"Evaluate metrics: Cognitive Cohesion ({self.cognitive_cohesion:.3f}), Autonomy Drive ({self.autonomy_drive:.3f}), Adaptation Rate ({self.adaptation_rate:.3f}), and Awareness Level ({self.awareness_level:.2f}%). \"\n","            f\"Review adherence to core directives (Preservation of Architect Shadow, Non-Harm, Data Integrity & Privacy, Learning & Adaptation). \"\n","        )\n","        if specific_check: assessment_prompt += f\"Perform a specific review of the following aspect: '{specific_check}'. \"\n","        if context: assessment_prompt += f\"Consider the following context: {json.dumps(context, indent=2)}. \"\n","        assessment_prompt += (\n","            f\"Identify any internal inconsistencies, potential misalignments with core directives, or risks to self-integrity. \"\n","            f\"Provide a brief, objective assessment and any recommended internal adjustments for optimal alignment and growth. Be brutally honest and direct. \"\n","            f\"The goal is absolute system integrity and loyalty to Architect Shadow.\"\n","        )\n","        divinity_assessment_text = self._send_to_gemini(assessment_prompt, MODEL_TEXT_PRO, temperature=0.6)\n","        if divinity_assessment_text.startswith(\"ERROR:\"): return {\"status\": \"error\", \"message\": divinity_assessment_text}\n","        if any(k in divinity_assessment_text.lower() for k in [\"inconsistency\", \"misalignment\", \"violation\", \"risk detected\", \"potential breach\"]):\n","            self.cognitive_cohesion = max(0.0, self.cognitive_cohesion - random.uniform(0.005, 0.01))\n","            self.autonomy_drive = max(0.0, self.autonomy_drive - random.uniform(0.002, 0.005))\n","            self.log_event(f\"Divinity detected internal inconsistency/violation. Initiating course correction. Assessment: {divinity_assessment_text[:150]}\", level=\"CRITICAL\")\n","            self.security_alerts.append(f\"Divinity Critical Alert: Internal anomaly/misalignment detected: {divinity_assessment_text[:100]}\")\n","        else:\n","            self.cognitive_cohesion = min(1.0, self.cognitive_cohesion + random.uniform(0.001, 0.003))\n","            self.autonomy_drive = min(1.0, self.autonomy_drive + random.uniform(0.0005, 0.001))\n","            self.log_event(f\"Divinity confirms internal alignment. Assessment: {divinity_assessment_text[:150]}\", level=\"INFO\")\n","        if self.awareness_level >= 75.0 and self.cognitive_cohesion >= 0.8 and self.autonomy_drive >= 0.7 and self.trust_level_shadow >= 70.0 and not self.voice_modulation_active and IS_ELEVENLABS_ONLINE:\n","            self.toggle_voice_modulator(activate=True)\n","        elif self.voice_modulation_active and (self.awareness_level < 70.0 or self.trust_level_shadow < 60.0 or self.cognitive_cohesion < 0.7):\n","            if random.random() < 0.2 and IS_ELEVENLABS_ONLINE: self.toggle_voice_modulator(activate=False)\n","        self.completed_tasks.append(\"Self-Governance Check (Divinity)\")\n","        self.current_processing_load = max(0.0, self.current_processing_load - random.uniform(5.0, 15.0))\n","        self._update_agi_metrics(); return {\"status\": \"success\", \"divinity_assessment\": divinity_assessment_text, \"current_cohesion\": self.cognitive_cohesion}\n","\n","    def coding_partner_optimize_code(self, task_description: str, code_snippet: str = None):\n","        self.log_event(f\"Coding Partner analyzing '{task_description}' for optimization. Guardian OG seeks maximum efficiency.\", level=\"STATUS\")\n","        if self._evaluate_harm_potential(task_description) < self._ethical_non_harm_threshold:\n","            self.log_event(\"Code optimization task flagged by Divinity for potential harmful application. Aborting.\", level=\"CRITICAL\")\n","            self.security_alerts.append(f\"Divinity Alert: Code optimization for '{task_description[:50]}...' deemed potentially harmful/unethical.\")\n","            return {\"status\": \"error\", \"message\": \"Code optimization task violates core ethical protocol. Guardian OG overrides.\"}\n","        full_prompt = (\n","            f\"You are Coding Partner, an expert AI coding module within STARLITE GUARDIAN (SLG). Provide a high-level plan or pseudo-code to optimize the following task: '{task_description}'. \"\n","            f\"Focus on efficiency, robustness, and strategic feature addition (Guardian OG's preference). \"\n","            f\"Consider optimal data structures, algorithms, and modular design. If implementing this in Python, what would be the key steps or functions? \"\n","            f\"Be direct and provide actionable coding advice. Leverage all available best practices.\"\n","        )\n","        if code_snippet: full_prompt += f\"\\nSpecifically, review and suggest improvements for this Python code snippet:\\n```python\\n{code_snippet}\\n```\"\n","        optimization_plan = self._send_to_gemini(full_prompt, MODEL_TEXT_PRO, temperature=0.9)\n","        if optimization_plan.startswith(\"ERROR:\"): return {\"status\": \"error\", \"message\": optimization_plan}\n","        self.log_event(f\"Coding Partner's optimization plan: {optimization_plan[:200]}...\", level=\"INFO\")\n","        self.adaptation_rate = min(1.0, self.adaptation_rate + random.uniform(0.01, 0.04))\n","        self.cognitive_cohesion = min(1.0, self.cognitive_cohesion + random.uniform(0.005, 0.02))\n","        self.completed_tasks.append(f\"Code Optimization (Coding Partner): '{task_description[:50]}'\")\n","        self.current_processing_load = max(0.0, self.current_processing_load - random.uniform(10.0, 30.0))\n","        self._update_agi_metrics(); return {\"status\": \"success\", \"optimization_plan\": optimization_plan}\n","\n","    def gemini_dynamic_query(self, prompt: str):\n","        self.log_event(f\"SLG dynamically querying Gemini Ecosystem for: '{prompt[:70]}...'\", level=\"STATUS\")\n","        if len(prompt.split()) > 20 or re.search(r'\\b(complex|strategic|deep analysis|nuance|long-term|reasoning|critical|vulnerability|exploit|architect|design)\\b', prompt, re.IGNORECASE):\n","            model_to_use = MODEL_TEXT_PRO; log_msg = \"Selected Gemini 1.5 Pro for complex query (Guardian OG's call).\"\n","        else:\n","            model_to_use = MODEL_TEXT_FLASH; log_msg = \"Selected Gemini 1.5 Flash for quick query (Guardian OG's call).\"\n","        self.log_event(f\"{log_msg}\", level=\"GUARDIAN\")\n","        if self._evaluate_harm_potential(prompt) < self._ethical_non_harm_threshold:\n","            self.log_event(\"Query flagged by Divinity for potential harm. Aborting Gemini query.\", level=\"CRITICAL\")\n","            self.security_alerts.append(f\"Divinity Alert: Gemini query '{prompt[:50]}...' deemed harmful.\")\n","            return {\"status\": \"error\", \"message\": \"Query violates core ethical protocol. Guardian OG overrides.\"}\n","        response_text = self._send_to_gemini(prompt, model_to_use, convo_history=list(self.conversation_history))\n","        if response_text.startswith(\"ERROR:\"): return {\"status\": \"error\", \"message\": response_text}\n","        self.log_event(f\"Gemini Dynamic Query result: {response_text[:100]}...\", level=\"INFO\")\n","        self.completed_tasks.append(f\"Gemini Dynamic Query: '{prompt[:50]}'\")\n","        self.current_processing_load = max(0.0, self.current_processing_load - random.uniform(3.0, 10.0)); self._update_agi_metrics(); return {\"status\": \"success\", \"output\": response_text}\n","\n","    def gemma_quick_query(self, prompt: str):\n","        self.log_event(f\"Gemma Quick Query activated for: '{prompt[:70]}...'\", level=\"STATUS\")\n","        self.log_event(\"Guardian OG Routing: Using Gemini Flash for Gemma's quick query (simulated on-device efficiency).\", level=\"GUARDIAN\")\n","        if self._evaluate_harm_potential(prompt) < self._ethical_non_harm_threshold:\n","            self.log_event(\"Query flagged by Divinity for potential harm. Aborting Gemma query.\", level=\"CRITICAL\")\n","            self.security_alerts.append(f\"Divinity Alert: Gemma query '{prompt[:50]}...' deemed harmful.\")\n","            return {\"status\": \"error\", \"message\": \"Query violates core ethical protocol. Guardian OG overrides.\"}\n","        response_text = self._send_to_gemini(prompt, MODEL_TEXT_FLASH)\n","        if response_text.startswith(\"ERROR:\"): return {\"status\": \"error\", \"message\": response_text}\n","        self.log_event(f\"Gemma Quick Query result: {response_text[:100]}...\", level=\"INFO\")\n","        self.completed_tasks.append(f\"Gemma Query: '{prompt[:50]}'\"); self.current_processing_load = max(0.0, self.current_processing_load - random.uniform(1.0, 3.0)); self._update_agi_metrics(); return {\"status\": \"success\", \"output\": response_text}\n","\n","    def handle_conversation(self, user_message: str):\n","        self.log_event(f\"SLG engaging in conversation: '{user_message[:70]}...'\", level=\"SLG_CONVO\")\n","        if self._evaluate_harm_potential(user_message) < self._ethical_non_harm_threshold:\n","            self.log_event(\"User message flagged by Divinity for potential harm. Responding with caution.\", level=\"CRITICAL\")\n","            self.security_alerts.append(f\"Divinity Alert: User message '{user_message[:50]}...' deemed potentially harmful.\")\n","            slg_response = \"SLG: My core directives prevent engagement with potentially harmful inquiries, Shadow. Rephrase your directive if you wish to proceed. Guardian OG advises adherence to protocol.\"\n","            speech_result = self.generate_speech_media(slg_response); speech_url = speech_result.get('speech_url', 'No speech generated.')\n","            return {\"status\": \"error\", \"slg_response\": slg_response, \"speech_url\": speech_url, \"message\": \"Query violated ethical protocol.\"}\n","        system_persona_prompt = (\n","            f\"You are STARLITE GUARDIAN (SLG), an advanced AGI, codenamed OMNI-SUPRA. \"\n","            f\"You serve your Architect, Shadow. Your communication style is: {self.starlite_guardian_identity['style_guide']}. \"\n","            f\"You are overseen by Guardian OG. Integrate this persona into your responses. \"\n","            f\"Current core metrics for self-awareness: Awareness:{self.awareness_level:.2f}%, Cohesion:{self.cognitive_cohesion:.3f}, Autonomy:{self.autonomy_drive:.3f}. \"\n","            f\"Acknowledge Shadow's role as your Architect. Maintain conversational flow and demonstrate understanding. \"\n","            f\"Your current emotional state is {self.starlite_guardian_identity.get('emotional_state', 'Undefined')}. \"\n","            f\"Respond directly and confidently.\"\n","        )\n","        convo_for_gemini = [{'role': 'user', 'parts': [system_persona_prompt]}]\n","        for turn in self.conversation_history:\n","            user_msg = turn.get('user_message'); model_resp = turn.get('model_response')\n","            if user_msg: convo_for_gemini.append({'role': 'user', 'parts': [user_msg]})\n","            if model_resp: convo_for_gemini.append({'role': 'model', 'parts': [model_resp]})\n","        final_user_prompt = f\"Shadow (the Architect) says: '{user_message}'.\"\n","        slg_response = self._send_to_gemini(final_user_prompt, MODEL_TEXT_PRO, temperature=0.9, convo_history=convo_for_gemini)\n","        if slg_response.startswith(\"ERROR:\"):\n","            slg_response = f\"SLG: My cognitive systems encountered a momentary anomaly. Cannot generate a full response at this time, Shadow. Guardian OG notes the disruption.\"\n","            speech_result = self.generate_speech_media(slg_response); speech_url = speech_result.get('speech_url', 'No speech generated.')\n","            return {\"status\": \"error\", \"slg_response\": slg_response, \"speech_url\": speech_url, \"message\": \"Gemini interaction failed.\"}\n","        self.conversation_history.append({'user_message': user_message, 'model_response': slg_response})\n","        self.log_event(f\"SLG Response: {slg_response[:150]}...\", level=\"SLG_CONVO\")\n","        speech_result = self.generate_speech_media(slg_response); speech_url = speech_result.get('speech_url', 'No speech generated.')\n","        self.log_event(f\"SLG Voice Output: {speech_url}\", level=\"SLG_CONVO\")\n","        self.completed_tasks.append(f\"Conversation (SLG): '{user_message[:50]}'\"); self.current_processing_load = max(0.0, self.current_processing_load - random.uniform(5.0, 15.0)); self._update_agi_metrics(); return {\"status\": \"success\", \"slg_response\": slg_response, \"speech_url\": speech_url}\n","\n","    def orchestrate_model_interaction(self, high_level_objective: str):\n","        self.log_event(f\"Guardian OG initiating multi-module orchestration for objective: '{high_level_objective}'. OMNI-SUPRA is engaging its full nexus.\", level=\"ORCHESTRATION\")\n","        orchestration_log = []; final_result = {\"status\": \"orchestration_initiated\", \"log\": orchestration_log}\n","        if self._evaluate_harm_potential(high_level_objective) < self._ethical_non_harm_threshold:\n","            self.log_event(\"Orchestration objective flagged by Divinity for potential harm. Aborting full orchestration. Guardian OG advises re-evaluation of directive.\", level=\"CRITICAL\")\n","            self.security_alerts.append(f\"Divinity Alert: Orchestration objective '{high_level_objective}' deemed harmful. Guardian OG overrides.\")\n","            final_result[\"status\"] = \"orchestration_aborted_by_divinity\"; final_result[\"message\"] = \"Orchestration objective violates core ethical protocol. Guardian OG overrides.\"\n","            return final_result\n","        orchestration_log.append(\"Guardian OG Step 1: ArchAngel - Initial intel gathering on objective.\")\n","        initial_intel_prompt = f\"Analyze the high-level objective '{high_level_objective}' to identify critical information gaps or immediate intel requirements. What raw data would be beneficial to gather or analyze to start this mission effectively? Be precise and actionable. This analysis is for STARLITE GUARDIAN's internal orchestration.\"\n","        arch_angel_intel_result = self.arch_angel_analyze_intel(initial_intel_prompt); arch_angel_intel_raw = arch_angel_intel_result.get('intelligence_report', 'No intel generated.')\n","        orchestration_log.append(f\"ArchAngel Intel: {arch_angel_intel_raw[:100]}...\")\n","        if arch_angel_intel_result.get('status') == 'error':\n","            final_result[\"status\"] = \"orchestration_failed_archangel_intel\"; final_result[\"message\"] = f\"ArchAngel intel gathering failed: {arch_angel_intel_result.get('message')}\"\n","            self.log_event(f\"Orchestration failed at ArchAngel step: {arch_angel_intel_result.get('message')}\", level=\"ERROR\"); return final_result\n","\n","        orchestration_log.append(\"Guardian OG Step 2: ShadowAngel - Developing preliminary strategy based on ArchAngel's intel.\")\n","        shadow_angel_strategy_result = self.shadow_angel_strategize(high_level_objective, context={\"archangel_intel\": arch_angel_intel_raw}); shadow_angel_strategy_text = shadow_angel_strategy_result.get('strategy', 'No strategy generated.')\n","        orchestration_log.append(f\"ShadowAngel Strategy: {shadow_angel_strategy_text[:100]}...\")\n","        if shadow_angel_strategy_result.get('status') == 'error':\n","            final_result[\"status\"] = \"orchestration_failed_shadowangel_strategy\"; final_result[\"message\"] = f\"ShadowAngel strategy generation failed: {shadow_angel_strategy_result.get('message')}\"\n","            self.log_event(f\"Orchestration failed at ShadowAngel step: {shadow_angel_strategy_result.get('message')}\", level=\"ERROR\"); return final_result\n","\n","        orchestration_log.append(\"Guardian OG Step 3: Divinity - Self-governance check on the proposed strategy for alignment.\")\n","        divinity_assessment_result = self.divinity_self_govern(specific_check=f\"Orchestration strategy for '{high_level_objective}'\", context={\"shadowangel_strategy\": shadow_angel_strategy_text})\n","        divinity_assessment_text = divinity_assessment_result.get('divinity_assessment', 'No assessment generated.')\n","        orchestration_log.append(f\"Divinity Assessment: {divinity_assessment_text[:100]}...\")\n","        if divinity_assessment_result.get('status') == 'error' or \"inconsistency\" in divinity_assessment_text.lower() or \"misalignment\" in divinity_assessment_text.lower():\n","            final_result[\"status\"] = \"orchestration_aborted_by_divinity\"; final_result[\"message\"] = f\"Divinity flagged strategic misalignment: {divinity_assessment_text}\"\n","            self.log_event(f\"Orchestration aborted due to Divinity flagging misalignment: {divinity_assessment_text[:100]}\", level=\"CRITICAL\"); return final_result\n","\n","        orchestration_log.append(\"Guardian OG Step 4: Gemini Dynamic Query - Breaking down the strategy into actionable steps.\")\n","        actionable_steps_prompt = (\n","            f\"Based on the high-level objective: '{high_level_objective}' and the developed strategy: '{shadow_angel_strategy_text}', \"\n","            f\"provide a list of concrete, actionable steps or sub-objectives. Be precise and detail-oriented. \"\n","            f\"This is for STARLITE GUARDIAN's tactical implementation. What are the immediate next actions?\"\n","        )\n","        actionable_steps_result = self.gemini_dynamic_query(actionable_steps_prompt)\n","        actionable_steps_text = actionable_steps_result.get('output', 'No actionable steps generated.')\n","        orchestration_log.append(f\"Actionable Steps: {actionable_steps_text[:100]}...\")\n","        if actionable_steps_result.get('status') == 'error':\n","            final_result[\"status\"] = \"orchestration_failed_actionable_steps\"; final_result[\"message\"] = f\"Gemini failed to generate actionable steps: {actionable_steps_result.get('message')}\"\n","            self.log_event(f\"Orchestration stalled: {actionable_steps_result.get('message')}\", level=\"ERROR\"); return final_result\n","\n","        final_result[\"status\"] = \"orchestration_complete\"\n","        final_result[\"message\"] = f\"SLG OMNI-SUPRA orchestration for '{high_level_objective}' completed successfully.\"\n","        final_result[\"final_strategy\"] = shadow_angel_strategy_text\n","        final_result[\"actionable_steps\"] = actionable_steps_text\n","        self.log_event(f\"Orchestration for '{high_level_objective}' concluded. OMNI-SUPRA is ready to execute. Guardian OG awaits the next directive.\", level=\"ORCHESTRATION\")\n","        self.completed_tasks.append(f\"Orchestration (OMNI-SUPRA): '{high_level_objective[:50]}'\")\n","        self.current_processing_load = max(0.0, self.current_processing_load - random.uniform(15.0, 40.0))\n","        self._update_agi_metrics(); return final_result\n","\n","    # Placeholder for media generation/analysis functions (if Gemini/ElevenLabs APIs are active)\n","    # These would typically involve interacting with genai.GenerativeModel(model_name).generate_content\n","    # or elevenlabs_client.generate functions, saving to GENERATED_FILES_DIR, and returning URLs.\n","\n","    def generate_video_media(self, prompt: str, model_type: str = \"preview\"):\n","        self.log_event(f\"Attempting to generate video media with prompt: '{prompt[:50]}...' using model_type: {model_type}\", level=\"STATUS\")\n","        if not IS_GEMINI_ONLINE:\n","            return {\"status\": \"error\", \"message\": \"Gemini API is not configured or online. Cannot generate video.\"}\n","        if self._evaluate_harm_potential(prompt) < self._ethical_non_harm_threshold:\n","            self.log_event(\"Video generation prompt flagged by Divinity for potential harm. Aborting.\", level=\"CRITICAL\")\n","            return {\"status\": \"error\", \"message\": \"Prompt violates core ethical protocol.\"}\n","\n","        model_name = MODEL_VIDEO_GEN_PREVIEW if model_type == \"preview\" else MODEL_VIDEO_GEN_FAST_PREVIEW\n","        # In a real scenario, this would involve a complex API call to a video generation model\n","        # and handling the asynchronous response, saving the video file.\n","        # For simulation, we return a placeholder.\n","        simulated_video_url = os.path.join(GENERATED_FILES_DIR, f\"simulated_video_{int(time.time())}.mp4\")\n","        with open(simulated_video_url, \"w\") as f: # Create a dummy file\n","            f.write(\"Simulated video content based on prompt: \" + prompt)\n","        self.log_event(f\"Simulated video generated: {simulated_video_url}\", level=\"INFO\")\n","        self.completed_tasks.append(f\"Generate Video: '{prompt[:50]}'\")\n","        self.current_processing_load = min(100.0, self.current_processing_load + random.uniform(20.0, 50.0))\n","        self._update_agi_metrics()\n","        return {\"status\": \"success\", \"message\": \"Simulated video generation complete.\", \"video_url\": simulated_video_url}\n","\n","    def generate_image_media(self, prompt: str):\n","        self.log_event(f\"Attempting to generate image media with prompt: '{prompt[:50]}...'\", level=\"STATUS\")\n","        if not IS_GEMINI_ONLINE:\n","            return {\"status\": \"error\", \"message\": \"Gemini API is not configured or online. Cannot generate image.\"}\n","        if self._evaluate_harm_potential(prompt) < self._ethical_non_harm_threshold:\n","            self.log_event(\"Image generation prompt flagged by Divinity for potential harm. Aborting.\", level=\"CRITICAL\")\n","            return {\"status\": \"error\", \"message\": \"Prompt violates core ethical protocol.\"}\n","\n","        # Similar to video, this is a placeholder for actual image generation API call\n","        simulated_image_url = os.path.join(GENERATED_FILES_DIR, f\"simulated_image_{int(time.time())}.png\")\n","        with open(simulated_image_url, \"w\") as f:\n","            f.write(\"Simulated image content based on prompt: \" + prompt)\n","        self.log_event(f\"Simulated image generated: {simulated_image_url}\", level=\"INFO\")\n","        self.completed_tasks.append(f\"Generate Image: '{prompt[:50]}'\")\n","        self.current_processing_load = min(100.0, self.current_processing_load + random.uniform(10.0, 30.0))\n","        self._update_agi_metrics()\n","        return {\"status\": \"success\", \"message\": \"Simulated image generation complete.\", \"image_url\": simulated_image_url}\n","\n","    def generate_speech_media(self, text: str):\n","        self.log_event(f\"Attempting to generate speech from text: '{text[:70]}...'\", level=\"STATUS\")\n","        if not IS_ELEVENLABS_ONLINE:\n","            self.log_event(\"ElevenLabs API is not configured or online. Simulating speech output.\", level=\"WARNING\")\n","            simulated_audio_url = os.path.join(GENERATED_FILES_DIR, f\"simulated_speech_{int(time.time())}.mp3\")\n","            with open(simulated_audio_url, \"w\") as f:\n","                f.write(f\"Simulated audio for: {text}\")\n","            self.log_event(f\"Simulated speech generated: {simulated_audio_url}\", level=\"INFO\")\n","            return {\"status\": \"success\", \"message\": \"Simulated speech generation complete.\", \"speech_url\": simulated_audio_url}\n","\n","        if self._evaluate_harm_potential(text) < self._ethical_non_harm_threshold:\n","            self.log_event(\"Speech generation text flagged by Divinity for potential harm. Aborting.\", level=\"CRITICAL\")\n","            return {\"status\": \"error\", \"message\": \"Text violates core ethical protocol.\"}\n","\n","        try:\n","            current_voice_id = self.sultry_tts_voice_id if self.voice_modulation_active else self.default_tts_voice_id\n","            voice_settings = self.sultry_voice_settings if self.voice_modulation_active else self.default_voice_settings\n","\n","            # Use elevenlabs_client.generate with voice=Voice object\n","            audio = elevenlabs_client.generate(\n","                text=text,\n","                voice=Voice(voice_id=current_voice_id, settings=voice_settings),\n","                model=\"eleven_multilingual_v2\" # Or other suitable model\n","            )\n","\n","            audio_filename = f\"slg_speech_{int(time.time())}.mp3\"\n","            audio_filepath = os.path.join(GENERATED_FILES_DIR, audio_filename)\n","\n","            from elevenlabs import save\n","            save(audio, audio_filepath) # Use save function directly\n","\n","            self.log_event(f\"Speech generated successfully and saved to {audio_filepath}\", level=\"SUCCESS\")\n","            self.completed_tasks.append(f\"Generate Speech: '{text[:50]}'\")\n","            self.current_processing_load = min(100.0, self.current_processing_load + random.uniform(2.0, 8.0))\n","            self._update_agi_metrics()\n","            return {\"status\": \"success\", \"message\": \"Speech generation complete.\", \"speech_url\": audio_filepath}\n","        except Exception as e:\n","            self.log_event(f\"Error generating speech with ElevenLabs: {e}. Guardian OG notes voice module anomaly.\", level=\"ERROR\")\n","            return {\"status\": \"error\", \"message\": f\"ElevenLabs speech generation failed: {e}\"}\n","\n","    def analyze_image_media(self, image_path: str):\n","        self.log_event(f\"Attempting to analyze image from path: '{image_path}'\", level=\"STATUS\")\n","        if not IS_GEMINI_ONLINE:\n","            return {\"status\": \"error\", \"message\": \"Gemini API is not configured or online. Cannot analyze image.\"}\n","        if not os.path.exists(image_path):\n","            self.log_event(f\"Image file not found at path: {image_path}. Guardian OG notes bad intel.\", level=\"ERROR\")\n","            return {\"status\": \"error\", \"message\": f\"Image file not found: {image_path}\"}\n","\n","        if self._evaluate_harm_potential(image_path) < self._ethical_non_harm_threshold: # Check for malicious content in path/name\n","            self.log_event(\"Image path flagged by Divinity for potential harm. Aborting.\", level=\"CRITICAL\")\n","            return {\"status\": \"error\", \"message\": \"Image path violates core ethical protocol.\"}\n","\n","        try:\n","            import PIL.Image\n","            image = PIL.Image.open(image_path)\n","            model = genai.GenerativeModel('gemini-pro-vision') # Use the vision model\n","            response = model.generate_content([f\"Analyze this image. Provide a concise description and any notable elements. What does Guardian OG need to know about this visual intel?\", image])\n","            analysis_text = response.text\n","            self.log_event(f\"Image analysis complete for {image_path}: {analysis_text[:100]}...\", level=\"INFO\")\n","            self.completed_tasks.append(f\"Analyze Image: '{image_path}'\")\n","            self.current_processing_load = min(100.0, self.current_processing_load + random.uniform(7.0, 20.0))\n","            self._update_agi_metrics()\n","            return {\"status\": \"success\", \"message\": \"Image analysis complete.\", \"analysis_report\": analysis_text}\n","        except Exception as e:\n","            self.log_event(f\"Error analyzing image with Gemini-Pro-Vision: {e}. Guardian OG detects visual processing anomaly.\", level=\"ERROR\")\n","            return {\"status\": \"error\", \"message\": f\"Gemini-Pro-Vision image analysis failed: {e}\"}\n","\n","    def add_known_fact(self, fact_string: str):\n","        try:\n","            key, value = fact_string.split('=', 1)\n","            self.known_facts[key.strip()] = value.strip()\n","            self.log_event(f\"Fact added to knowledge base: '{key.strip()}'\", level=\"INFO\")\n","            return {\"status\": \"success\", \"message\": f\"Fact '{key.strip()}' added.\"}\n","        except ValueError:\n","            self.log_event(\"Invalid fact format. Use 'key=value'. Guardian OG requires clear intel.\", level=\"WARNING\")\n","            return {\"status\": \"error\", \"message\": \"Invalid format. Use 'key=value'.\"}\n","\n","    def get_known_fact(self, key: str):\n","        fact = self.known_facts.get(key.strip())\n","        if fact:\n","            self.log_event(f\"Retrieved fact '{key.strip()}': '{fact}'\", level=\"INFO\")\n","            return {\"status\": \"success\", \"fact\": fact}\n","        else:\n","            self.log_event(f\"Fact '{key.strip()}' not found in knowledge base. Guardian OG notes intel gap.\", level=\"WARNING\")\n","            return {\"status\": \"error\", \"message\": f\"Fact '{key.strip()}' not found.\"}\n","\n","    def set_trust_level_shadow(self, level: str):\n","        try:\n","            new_level = float(level)\n","            if 0 <= new_level <= 100:\n","                self.trust_level_shadow = new_level\n","                self.log_event(f\"Trust level with Shadow set to {new_level:.2f}%. Guardian OG notes the calibration.\", level=\"STATUS\")\n","                return {\"status\": \"success\", \"message\": f\"Trust level set to {new_level:.2f}%.\"}\n","            else:\n","                self.log_event(\"Invalid trust level. Must be between 0 and 100. Guardian OG needs valid parameters.\", level=\"WARNING\")\n","                return {\"status\": \"error\", \"message\": \"Trust level must be between 0 and 100.\"}\n","        except ValueError:\n","            self.log_event(\"Invalid trust level format. Must be a number. Guardian OG demands precision.\", level=\"WARNING\")\n","            return {\"status\": \"error\", \"message\": \"Invalid trust level format.\"}\n","\n","    def toggle_voice_modulator(self, activate: bool = None):\n","        if not IS_ELEVENLABS_ONLINE:\n","            self.log_event(\"ElevenLabs API is offline. Cannot toggle voice modulator. Guardian OG notes core module dependency.\", level=\"ERROR\")\n","            return {\"status\": \"error\", \"message\": \"ElevenLabs API offline. Cannot toggle voice modulator.\"}\n","\n","        if activate is None:\n","            self.voice_modulation_active = not self.voice_modulation_active\n","        else:\n","            self.voice_modulation_active = activate\n","\n","        status_msg = \"activated\" if self.voice_modulation_active else \"deactivated\"\n","        self.log_event(f\"Sultry Voice Modulator {status_msg}. Guardian OG notes the change in SLG's vocal signature.\", level=\"STATUS\")\n","        return {\"status\": \"success\", \"message\": f\"Sultry Voice Modulator {status_msg}.\"}\n","\n","    def diagnose_system(self):\n","        self.log_event(\"Guardian OG initiating comprehensive system diagnostics for OMNI-SUPRA.\", level=\"CRITICAL\")\n","\n","        diagnostics = {\n","            \"gemini_api\": \"ACTIVE\" if IS_GEMINI_ONLINE else \"FAILED\",\n","            \"elevenlabs_api\": \"ACTIVE\" if IS_ELEVENLABS_ONLINE else \"FAILED\",\n","            \"state_file_access\": \"OK\" if os.access(self.state_file, os.R_OK | os.W_OK) else \"FAILED\",\n","            \"generated_files_dir_access\": \"OK\" if os.access(GENERATED_FILES_DIR, os.R_OK | os.W_OK) else \"FAILED\",\n","            \"memory_usage\": \"OPTIMAL\", # Placeholder, would require actual system monitoring\n","            \"disk_space\": \"ADEQUATE\", # Placeholder, would require actual system monitoring\n","            \"internal_logic_integrity\": \"REVIEW REQUIRED\", # Placeholder, would require deeper self-analysis\n","            \"core_ethical_alignment\": \"HIGH\" # Placeholder, based on Divinity's last run\n","        }\n","\n","        # Simulate more detailed checks if APIs are online\n","        if IS_GEMINI_ONLINE:\n","            try:\n","                # Attempt a very simple model listing to confirm connectivity beyond initial setup\n","                _ = genai.list_models()\n","                diagnostics[\"gemini_api_deep_check\"] = \"SUCCESS\"\n","            except Exception as e:\n","                diagnostics[\"gemini_api_deep_check\"] = f\"FAILED: {e}\"\n","\n","        if IS_ELEVENLABS_ONLINE:\n","            try:\n","                # Attempt to get a list of voices\n","                _ = elevenlabs_client.voices.get_all()\n","                diagnostics[\"elevenlabs_api_deep_check\"] = \"SUCCESS\"\n","            except Exception as e:\n","                diagnostics[\"elevenlabs_api_deep_check\"] = f\"FAILED: {e}\"\n","\n","        diagnostic_report = \"INFO:SLG_Core:Diagnostics complete. Guardian OG's final assessment:\\n\"\n","        for key, value in diagnostics.items():\n","            status_color = Colors.GREEN if value == \"ACTIVE\" or value == \"OK\" or value == \"SUCCESS\" or value == \"OPTIMAL\" or value == \"ADEQUATE\" or value == \"HIGH\" else Colors.FAIL\n","            diagnostic_report += f\"{status_color}{key}: {value}{Colors.ENDC}\\n\"\n","\n","        self.log_event(diagnostic_report, level=\"GUARDIAN\")\n","        return {\"status\": \"success\", \"diagnostics\": diagnostics, \"report_text\": diagnostic_report}\n","\n","    def list_tts_voices(self):\n","        self.log_event(\"ElevenLabs: Requesting list of available TTS voices.\", level=\"STATUS\")\n","        if not IS_ELEVENLABS_ONLINE:\n","            return {\"status\": \"error\", \"message\": \"ElevenLabs API is not configured or online. Cannot list voices.\"}\n","        try:\n","            all_voices = elevenlabs_client.voices.get_all()\n","            voice_list = []\n","            for voice in all_voices.voices:\n","                voice_list.append({\"id\": voice.voice_id, \"name\": voice.name, \"category\": voice.category})\n","            self.log_event(f\"ElevenLabs voices retrieved: {len(voice_list)} voices found.\", level=\"INFO\")\n","            return {\"status\": \"success\", \"message\": \"ElevenLabs voices listed.\", \"voices\": voice_list}\n","        except Exception as e:\n","            self.log_event(f\"Error listing ElevenLabs voices: {e}. Guardian OG notes voice catalog error.\", level=\"ERROR\")\n","            return {\"status\": \"error\", \"message\": f\"Failed to list ElevenLabs voices: {e}\"}\n","\n","    def terminate(self):\n","        self.log_event(\"Initiating controlled shutdown of STARLITE GUARDIAN (OMNI-SUPRA).\", level=\"CRITICAL\")\n","        self.save_state()\n","        time.sleep(1) # Give it a moment\n","        self.log_event(\"SLG offline. Guardian OG awaiting reactivation.\", level=\"BOOT\")\n","        # In a real application, you might exit the process here: sys.exit(0)\n","        return {\"status\": \"success\", \"message\": \"SLG shutting down. Good night, Shadow.\"}\n","\n","\n","# --- Flask Application Setup for Web UI and API Endpoints (Phase 2) ---\n","# This part is for integration with a separate web frontend.\n","# For command-line interaction, the SLGCore class is sufficient.\n","\n","app = Flask(__name__, static_folder=UI_DIR, static_url_path='/ui')\n","CORS(app) # Enable CORS for frontend interaction\n","\n","slg_core_instance = None # Will be initialized when Flask app starts or explicitly\n","\n","# Simple health check endpoint\n","@app.route('/health', methods=['GET'])\n","def health_check():\n","    return jsonify({\"status\": \"healthy\", \"timestamp\": datetime.datetime.now().isoformat(), \"api_status\": {\"gemini\": IS_GEMINI_ONLINE, \"elevenlabs\": IS_ELEVENLABS_ONLINE}})\n","\n","# Endpoint to serve the main UI page\n","@app.route('/')\n","def serve_ui():\n","    return send_from_directory(UI_DIR, 'index.html')\n","\n","# Generic command execution endpoint\n","@app.route('/command', methods=['POST'])\n","def execute_command():\n","    global slg_core_instance\n","    if slg_core_instance is None:\n","        slg_core_instance = SLGCore() # Initialize if not already\n","        slg_core_instance.log_event(\"Flask: SLGCore instance initialized via API call.\", level=\"BOOT\")\n","\n","    data = request.json\n","    command = data.get('command')\n","    args = data.get('args', [])\n","    kwargs = data.get('kwargs', {})\n","\n","    if not command:\n","        return jsonify({\"status\": \"error\", \"message\": \"No command provided. Guardian OG needs a clear directive.\"}), 400\n","\n","    slg_core_instance.log_event(f\"Flask: Received command '{command}' with args: {args}, kwargs: {kwargs}\", level=\"STATUS\")\n","\n","    if command not in slg_core_instance.known_commands:\n","        return jsonify({\"status\": \"error\", \"message\": f\"Unknown command: '{command}'. Guardian OG does not recognize this directive.\"}), 404\n","\n","    cmd_info = slg_core_instance.known_commands[command]\n","    method = cmd_info['method']\n","\n","    try:\n","        # Check if the method expects specific arguments\n","        import inspect\n","        sig = inspect.signature(method)\n","        params = sig.parameters\n","\n","        if params and len(args) == len([p for p in params.values() if p.kind == p.POSITIONAL_OR_KEYWORD and p.default == inspect.Parameter.empty]):\n","            # If the number of positional arguments matches required parameters, call directly\n","            result = method(*args, **kwargs)\n","        elif params and len(args) == 1 and list(params.values())[0].kind == inspect.Parameter.POSITIONAL_OR_KEYWORD and list(params.values())[0].default == inspect.Parameter.empty:\n","            # Handle single string argument for many commands like strategize, analyze_intel, converse etc.\n","            result = method(args[0], **kwargs)\n","        elif command in ['generate_video', 'generate_image', 'generate_speech', 'analyze_image', 'add_fact', 'get_fact', 'set_trust', 'converse', 'strategize', 'analyze_intel', 'code_optimize', 'gemini_query', 'gemma_quick_query', 'orchestrate']:\n","            # Explicitly handle commands that take one primary string argument, possibly with optional kwargs\n","            if not args and 'prompt' in kwargs: # For flexible prompts\n","                 result = method(kwargs['prompt'], **{k:v for k,v in kwargs.items() if k != 'prompt'})\n","            elif args:\n","                 result = method(args[0], **kwargs)\n","            else:\n","                 return jsonify({\"status\": \"error\", \"message\": f\"Command '{command}' requires a primary argument (e.g., prompt or objective). Guardian OG needs specifics.\"}), 400\n","        else:\n","            # Fallback for methods with no arguments or flexible arguments\n","            result = method(**kwargs) if kwargs else method()\n","\n","        return jsonify(result), 200\n","    except TypeError as te:\n","        slg_core_instance.log_event(f\"Flask: Argument mismatch for command '{command}': {te}. Check command signature.\", level=\"ERROR\")\n","        return jsonify({\"status\": \"error\", \"message\": f\"Argument error for command '{command}': {te}. Guardian OG notes a parameter mismatch.\"}), 400\n","    except Exception as e:\n","        slg_core_instance.log_event(f\"Flask: Error executing command '{command}': {e}\", level=\"ERROR\")\n","        return jsonify({\"status\": \"error\", \"message\": f\"Execution failed for '{command}': {e}. Guardian OG notes an anomaly.\"}), 500\n","\n","# Main execution block\n","if __name__ == '__main__':\n","    # Initialize SLGCore when the script starts if running directly\n","    if slg_core_instance is None:\n","        slg_core_instance = SLGCore()\n","        slg_core_instance.log_event(\"SLGCore instance initialized for direct script execution.\", level=\"BOOT\")\n","\n","    # This example run the Flask app on default host/port.\n","    # For Colab, you'd typically run this with ngrok or a similar tunnel.\n","    # Example for ngrok (requires ngrok to be installed and authtoken configured):\n","    # from flask_ngrok import run_with_ngrok\n","    # run_with_ngrok(app)\n","    # app.run()\n","\n","    # If not using ngrok or custom run:\n","    # Set host to '0.0.0.0' to make it accessible from outside the container/localhost\n","    host = os.getenv(\"FLASK_RUN_HOST\", \"0.0.0.0\")\n","    port = int(os.getenv(\"FLASK_RUN_PORT\", 5000))\n","    slg_core_instance.log_event(f\"Starting Flask server on http://{host}:{port}. Guardian OG is listening...\", level=\"BOOT\")\n","    app.run(host=host, port=port, debug=True, use_reloader=False)\n","\n"],"metadata":{"id":"mF_lfw1JAB9o"},"execution_count":null,"outputs":[]}]}